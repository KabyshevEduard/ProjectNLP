{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2ebf1dcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "from sklearn import preprocessing\n",
    "from sklearn import metrics\n",
    "from sklearn import model_selection\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import natasha\n",
    "from navec import Navec\n",
    "from slovnet.model.emb import NavecEmbedding\n",
    "import numpy as np\n",
    "import nltk\n",
    "import string\n",
    "import re\n",
    "from gensim.models import Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "71197ae3",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Эдуард\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Эдуард\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "path = 'navec_hudlit_v1_12B_500K_300d_100q.tar'\n",
    "navec = Navec.load(path)\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bf08e5ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "path_df = 'C:/Users/Эдуард/Desktop/проект1/data.txt'\n",
    "sep = ';'\n",
    "encoding = 'cp1251'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a4caeb9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(path_df, sep=sep, encoding=encoding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b10c43e4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Описание анкеты</th>\n",
       "      <th>Рекомендовать</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Ищу интересных людей. Люблю смотреть дорамы. О...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Пока тут</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Я теряюсь, когда меня просят рассказать о себе...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Хочу такую же кепку, как у Марио... Кто-нибудь...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Ищем с кем потусить, нас трое</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>191</th>\n",
       "      <td>Хочется найти увлечённого, ответственного, пор...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>192</th>\n",
       "      <td>Есть парень. Здесь хотела бы найти друзей, ком...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>193</th>\n",
       "      <td>Хочется добрых людей , приятных ,душевных поез...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>194</th>\n",
       "      <td>Хочется найти хорошего друга или подругу, може...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>195</th>\n",
       "      <td>Работаю, учусь и все такое. Нравится все матем...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>196 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                       Описание анкеты   Рекомендовать\n",
       "0    Ищу интересных людей. Люблю смотреть дорамы. О...               0\n",
       "1                                             Пока тут               0\n",
       "2    Я теряюсь, когда меня просят рассказать о себе...               0\n",
       "3    Хочу такую же кепку, как у Марио... Кто-нибудь...               0\n",
       "4                        Ищем с кем потусить, нас трое               0\n",
       "..                                                 ...             ...\n",
       "191  Хочется найти увлечённого, ответственного, пор...               0\n",
       "192  Есть парень. Здесь хотела бы найти друзей, ком...               0\n",
       "193  Хочется добрых людей , приятных ,душевных поез...               0\n",
       "194  Хочется найти хорошего друга или подругу, може...               0\n",
       "195  Работаю, учусь и все такое. Нравится все матем...               1\n",
       "\n",
       "[196 rows x 2 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b7d93d9f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda', index=0)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7359e42f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "44\n"
     ]
    }
   ],
   "source": [
    "text = df['Описание анкеты']\n",
    "print(len(max(text, key=len).split(' ')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c221eea0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(df, tokenize=False):\n",
    "    if tokenize:\n",
    "        dictionary = {}\n",
    "        vocab = []\n",
    "        k = 0\n",
    "        for i in range(len(df['Описание анкеты'].values)):\n",
    "            disc = df['Описание анкеты'].values[i]\n",
    "            tokens = nltk.word_tokenize(disc, language='russian')\n",
    "            for j in tokens:\n",
    "                if j in dictionary:\n",
    "                    pass\n",
    "                else:\n",
    "                    dictionary[j] = k\n",
    "                    vocab.append(j)\n",
    "                    k += 1\n",
    "        dictionary['<unk>'] = 0\n",
    "        return dictionary, vocab\n",
    "    else:\n",
    "        for i in range(len(df['Описание анкеты'].values)):\n",
    "            disc = df['Описание анкеты'].values[i].lower()\n",
    "            tokens = nltk.word_tokenize(disc, language='russian')\n",
    "            tokens_without_punct = [str(token) for token in tokens if token not in string.punctuation]\n",
    "            without_stop_words = [str(token) for token in tokens_without_punct if token not in nltk.corpus.stopwords.words('russian')]\n",
    "            without_specials = [re.sub(r'[^а-яА-ЯёЁ\\s]', '', word) for word in without_stop_words]\n",
    "            #стемминг\n",
    "            snowball = nltk.stem.SnowballStemmer(language=\"russian\")\n",
    "            stemmed = [snowball.stem(word) for word in without_specials]\n",
    "            #\n",
    "            clean = [j for j in stemmed if j != '']\n",
    "            df.loc[i, 'Описание анкеты'] = ' '.join(clean)\n",
    "        df = df[df['Описание анкеты'].map(len) <= 500]\n",
    "        return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c7e8c6ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_data = preprocess(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "83a02dec",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Описание анкеты</th>\n",
       "      <th>Рекомендовать</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ищ интересн люд любл смотрет дорам остальн общен</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>пок</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>теря прос рассказа лс уч умира подготовк дипл ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>хоч так кепк мар ктонибуд знает так взят любл ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ищ кем потус тро</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>191</th>\n",
       "      <td>хочет найт увлечен ответствен порядочн серьезн...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>192</th>\n",
       "      <td>парен хотел найт друз компан игра настолк глав...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>193</th>\n",
       "      <td>хочет добр люд приятн душевн поездок тепл разг...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>194</th>\n",
       "      <td>хочет найт хорош друг подруг компан друз душев...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>195</th>\n",
       "      <td>работа уч так нрав математическ любл разн музы...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>196 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                       Описание анкеты   Рекомендовать\n",
       "0     ищ интересн люд любл смотрет дорам остальн общен               0\n",
       "1                                                  пок               0\n",
       "2    теря прос рассказа лс уч умира подготовк дипл ...               0\n",
       "3    хоч так кепк мар ктонибуд знает так взят любл ...               0\n",
       "4                                     ищ кем потус тро               0\n",
       "..                                                 ...             ...\n",
       "191  хочет найт увлечен ответствен порядочн серьезн...               0\n",
       "192  парен хотел найт друз компан игра настолк глав...               0\n",
       "193  хочет добр люд приятн душевн поездок тепл разг...               0\n",
       "194  хочет найт хорош друг подруг компан друз душев...               0\n",
       "195  работа уч так нрав математическ любл разн музы...               1\n",
       "\n",
       "[196 rows x 2 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "90be0160",
   "metadata": {},
   "outputs": [],
   "source": [
    "dictionary, vocab = preprocess(df_data, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "af06d8ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "def words_for_training(df):\n",
    "    sentenses = []\n",
    "    for i in range(len(df)):\n",
    "        sentense = df['Описание анкеты'].values[i]\n",
    "        sentenses.append(sentense.split(' '))\n",
    "    return sentenses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "58eee2a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_text = words_for_training(df_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "1727f665",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'любл': 0,\n",
       " 'ищ': 1,\n",
       " 'общен': 2,\n",
       " 'прост': 3,\n",
       " 'уч': 4,\n",
       " 'чита': 5,\n",
       " 'найт': 6,\n",
       " 'гуля': 7,\n",
       " 'человек': 8,\n",
       " 'хоч': 9,\n",
       " 'аним': 10,\n",
       " 'пиш': 11,\n",
       " 'друг': 12,\n",
       " 'игра': 13,\n",
       " 'слуша': 14,\n",
       " 'музык': 15,\n",
       " 'рост': 16,\n",
       " 'смотр': 17,\n",
       " 'жизн': 18,\n",
       " 'интересн': 19,\n",
       " 'увлека': 20,\n",
       " 'смотрет': 21,\n",
       " 'погуля': 22,\n",
       " 'друз': 23,\n",
       " 'хочет': 24,\n",
       " 'пообща': 25,\n",
       " 'книг': 26,\n",
       " 'люд': 27,\n",
       " 'эт': 28,\n",
       " 'обща': 29,\n",
       " 'очен': 30,\n",
       " 'котор': 31,\n",
       " 'фильм': 32,\n",
       " 'врем': 33,\n",
       " 'весел': 34,\n",
       " 'работа': 35,\n",
       " 'отношен': 36,\n",
       " 'мог': 37,\n",
       " 'парн': 38,\n",
       " 'высок': 39,\n",
       " 'мальчик': 40,\n",
       " 'занима': 41,\n",
       " 'геншин': 42,\n",
       " 'пив': 43,\n",
       " 'поговор': 44,\n",
       " 'так': 45,\n",
       " 'скучн': 46,\n",
       " 'хотел': 47,\n",
       " 'нов': 48,\n",
       " 'подруг': 49,\n",
       " 'классн': 50,\n",
       " 'немн': 51,\n",
       " 'сво': 52,\n",
       " 'буд': 53,\n",
       " 'манг': 54,\n",
       " 'спат': 55,\n",
       " 'гитар': 56,\n",
       " 'такж': 57,\n",
       " 'игр': 58,\n",
       " 'люб': 59,\n",
       " 'сериал': 60,\n",
       " 'собеседник': 61,\n",
       " 'перв': 62,\n",
       " 'см': 63,\n",
       " 'обожа': 64,\n",
       " 'ещ': 65,\n",
       " 'жив': 66,\n",
       " 'приятн': 67,\n",
       " 'компан': 68,\n",
       " 'ком': 69,\n",
       " 'кур': 70,\n",
       " 'пью': 71,\n",
       " 'хим': 72,\n",
       " 'разговор': 73,\n",
       " 'истор': 74,\n",
       " 'дел': 75,\n",
       " 'сам': 76,\n",
       " 'нрав': 77,\n",
       " 'волейбол': 78,\n",
       " 'готов': 79,\n",
       " 'сюд': 80,\n",
       " 'больш': 81,\n",
       " 'спорт': 82,\n",
       " 'ваш': 83,\n",
       " 'пошл': 84,\n",
       " 'кажд': 85,\n",
       " 'дава': 86,\n",
       " 'мозг': 87,\n",
       " 'мно': 88,\n",
       " 'дом': 89,\n",
       " 'довольн': 90,\n",
       " 'мож': 91,\n",
       " 'зна': 92,\n",
       " 'настроен': 93,\n",
       " 'кин': 94,\n",
       " 'вкусн': 95,\n",
       " 'времен': 96,\n",
       " 'знакомств': 97,\n",
       " 'корейск': 98,\n",
       " 'стих': 99,\n",
       " 'интерес': 100,\n",
       " 'кпоп': 101,\n",
       " 'кем': 102,\n",
       " 'душ': 103,\n",
       " 'добр': 104,\n",
       " 'пок': 105,\n",
       " 'писа': 106,\n",
       " 'дорам': 107,\n",
       " 'свободн': 108,\n",
       " 'пабг': 109,\n",
       " 'ум': 110,\n",
       " 'рисова': 111,\n",
       " 'видосик': 112,\n",
       " 'сда': 113,\n",
       " 'редк': 114,\n",
       " 'нуж': 115,\n",
       " 'танц': 116,\n",
       " 'пит': 117,\n",
       " 'кемт': 118,\n",
       " 'крайност': 119,\n",
       " 'главн': 120,\n",
       " 'поигра': 121,\n",
       " 'сердечк': 122,\n",
       " 'нача': 123,\n",
       " 'вредн': 124,\n",
       " 'хож': 125,\n",
       " 'посмотр': 126,\n",
       " 'эмоциональн': 127,\n",
       " 'лет': 128,\n",
       " 'ден': 129,\n",
       " 'пицц': 130,\n",
       " 'ча': 131,\n",
       " 'вмест': 132,\n",
       " 'об': 133,\n",
       " 'животн': 134,\n",
       " 'всем': 135,\n",
       " 'громк': 136,\n",
       " 'смея': 137,\n",
       " 'галер': 138,\n",
       " 'дальш': 139,\n",
       " 'прот': 140,\n",
       " 'мал': 141,\n",
       " 'знает': 142,\n",
       " 'шар': 143,\n",
       " 'тактильн': 144,\n",
       " 'поступа': 145,\n",
       " 'вообщ': 146,\n",
       " 'фотограф': 147,\n",
       " 'част': 148,\n",
       " 'рот': 149,\n",
       " 'подар': 150,\n",
       " 'цветочк': 151,\n",
       " 'общительн': 152,\n",
       " 'ген': 153,\n",
       " 'город': 154,\n",
       " 'обнима': 155,\n",
       " 'комфортн': 156,\n",
       " 'театр': 157,\n",
       " 'характер': 158,\n",
       " 'мил': 159,\n",
       " 'лучш': 160,\n",
       " 'увлечен': 161,\n",
       " 'рис': 162,\n",
       " 'чтот': 163,\n",
       " 'теб': 164,\n",
       " 'перееха': 165,\n",
       " 'сидет': 166,\n",
       " 'провест': 167,\n",
       " 'симпатичн': 168,\n",
       " 'интроверт': 169,\n",
       " 'ког': 170,\n",
       " 'скидыва': 171,\n",
       " 'подобн': 172,\n",
       " 'душевн': 173,\n",
       " 'час': 174,\n",
       " 'разговарива': 175,\n",
       " 'рад': 176,\n",
       " 'остальн': 177,\n",
       " 'надежд': 178,\n",
       " 'лс': 179,\n",
       " 'зз': 180,\n",
       " 'литератур': 181,\n",
       " 'тепл': 182,\n",
       " 'физик': 183,\n",
       " 'вяза': 184,\n",
       " 'расскаж': 185,\n",
       " 'язык': 186,\n",
       " 'болта': 187,\n",
       " 'жит': 188,\n",
       " 'детк': 189,\n",
       " 'основн': 190,\n",
       " 'гот': 191,\n",
       " 'крут': 192,\n",
       " 'рисован': 193,\n",
       " 'молок': 194,\n",
       " 'пар': 195,\n",
       " 'блин': 196,\n",
       " 'творческ': 197,\n",
       " 'хорош': 198,\n",
       " 'полезн': 199,\n",
       " 'совершеннолетн': 200,\n",
       " 'родител': 201,\n",
       " 'ах': 202,\n",
       " 'еб': 203,\n",
       " 'спидкубинг': 204,\n",
       " 'выставля': 205,\n",
       " 'здравств': 206,\n",
       " 'побрынька': 207,\n",
       " 'зал': 208,\n",
       " 'тро': 209,\n",
       " 'ноч': 210,\n",
       " 'широк': 211,\n",
       " 'кругозор': 212,\n",
       " 'слегк': 213,\n",
       " 'пыта': 214,\n",
       " 'дипл': 215,\n",
       " 'каза': 216,\n",
       " 'малявк': 217,\n",
       " 'фон': 218,\n",
       " 'подготовк': 219,\n",
       " 'посвяща': 220,\n",
       " 'мангуманхв': 221,\n",
       " 'умира': 222,\n",
       " 'рассказа': 223,\n",
       " 'прос': 224,\n",
       " 'теря': 225,\n",
       " 'варкрафт': 226,\n",
       " 'варфрейм': 227,\n",
       " 'дестен': 228,\n",
       " 'том': 229,\n",
       " 'готовл': 230,\n",
       " 'кепк': 231,\n",
       " 'палк': 232,\n",
       " 'экстраверт': 233,\n",
       " 'мелодрам': 234,\n",
       " 'потус': 235,\n",
       " 'кемнибуд': 236,\n",
       " 'взят': 237,\n",
       " 'быва': 238,\n",
       " 'скачк': 239,\n",
       " 'рокметалл': 240,\n",
       " 'таролог': 241,\n",
       " 'жгуч': 242,\n",
       " 'радф': 243,\n",
       " 'лесбиянк': 244,\n",
       " 'ктонибуд': 245,\n",
       " 'гогол': 246,\n",
       " 'чу': 247,\n",
       " 'воня': 248,\n",
       " 'мар': 249,\n",
       " 'японск': 250,\n",
       " 'горн': 251,\n",
       " 'девочк': 252,\n",
       " 'заебыва': 253,\n",
       " 'дума': 254,\n",
       " 'казан': 255,\n",
       " 'род': 256,\n",
       " 'макеевк': 257,\n",
       " 'редс': 258,\n",
       " 'калинкин': 259,\n",
       " 'дела': 260,\n",
       " 'снача': 261,\n",
       " 'выслуша': 262,\n",
       " 'тво': 263,\n",
       " 'еба': 264,\n",
       " 'приветик': 265,\n",
       " 'заход': 266,\n",
       " 'интернет': 267,\n",
       " 'чипс': 268,\n",
       " 'бсд': 269,\n",
       " 'прив': 270,\n",
       " 'манхв': 271,\n",
       " 'тп': 272,\n",
       " 'катан': 273,\n",
       " 'фигурн': 274,\n",
       " 'кошк': 275,\n",
       " 'тик': 276,\n",
       " 'мор': 277,\n",
       " 'уснеш': 278,\n",
       " 'рассвет': 279,\n",
       " 'вид': 280,\n",
       " 'как': 281,\n",
       " 'берег': 282,\n",
       " 'сто': 283,\n",
       " 'видос': 284,\n",
       " 'ток': 285,\n",
       " 'би': 286,\n",
       " 'дс': 287,\n",
       " 'продл': 288,\n",
       " 'двух': 289,\n",
       " 'дне': 290,\n",
       " 'вопрос': 291,\n",
       " 'рыб': 292,\n",
       " 'ктнхп': 293,\n",
       " 'рыбызащитник': 294,\n",
       " 'татарочк': 295,\n",
       " 'европ': 296,\n",
       " 'тт': 297,\n",
       " 'фонк': 298,\n",
       " 'лож': 299,\n",
       " 'английск': 300,\n",
       " 'когот': 301,\n",
       " 'дот': 302,\n",
       " 'привычек': 303,\n",
       " 'красив': 304,\n",
       " 'русск': 305,\n",
       " 'христианин': 306,\n",
       " 'уровен': 307,\n",
       " 'задрот': 308,\n",
       " 'отъеб': 309,\n",
       " 'микас': 310,\n",
       " 'начальн': 311,\n",
       " 'волос': 312,\n",
       " 'скор': 313,\n",
       " 'рыж': 314,\n",
       " 'собира': 315,\n",
       " 'итис': 316,\n",
       " 'пупсик': 317,\n",
       " 'душн': 318,\n",
       " 'вес': 319,\n",
       " 'курсед': 320,\n",
       " '': 321,\n",
       " 'понемног': 322,\n",
       " 'дан': 323,\n",
       " 'момент': 324,\n",
       " 'блю': 325,\n",
       " 'лок': 326,\n",
       " 'ветрол': 327,\n",
       " 'пойдет': 328,\n",
       " 'акум': 329,\n",
       " 'пейзаж': 330,\n",
       " 'мб': 331,\n",
       " 'блог': 332,\n",
       " 'подпиш': 333,\n",
       " 'позна': 334,\n",
       " 'точн': 335,\n",
       " 'воздух': 336,\n",
       " 'свеж': 337,\n",
       " 'поэт': 338,\n",
       " 'ранг': 339,\n",
       " 'мем': 340,\n",
       " 'поддерж': 341,\n",
       " 'купл': 342,\n",
       " 'посво': 343,\n",
       " 'равн': 344,\n",
       " 'говор': 345,\n",
       " 'сложн': 346,\n",
       " 'вполн': 347,\n",
       " 'стран': 348,\n",
       " 'наивн': 349,\n",
       " 'степен': 350,\n",
       " 'какойт': 351,\n",
       " 'искрен': 352,\n",
       " 'одноразк': 353,\n",
       " 'фотографир': 354,\n",
       " 'песн': 355,\n",
       " 'по': 356,\n",
       " 'синтезатор': 357,\n",
       " 'суд': 358,\n",
       " 'цен': 359,\n",
       " 'сознательн': 360,\n",
       " 'взаимн': 361,\n",
       " 'воен': 362,\n",
       " 'изуча': 363,\n",
       " 'личност': 364,\n",
       " 'алкашк': 365,\n",
       " 'машин': 366,\n",
       " 'симпат': 367,\n",
       " 'обнимашк': 368,\n",
       " 'странност': 369,\n",
       " 'всяк': 370,\n",
       " 'коф': 371,\n",
       " 'тяжел': 372,\n",
       " 'татушк': 373,\n",
       " 'хот': 374,\n",
       " 'чудаковат': 375,\n",
       " 'обсужда': 376,\n",
       " 'каф': 377,\n",
       " 'мед': 378,\n",
       " 'куша': 379,\n",
       " 'приоритет': 380,\n",
       " 'инст': 381,\n",
       " 'штук': 382,\n",
       " 'вязан': 383,\n",
       " 'магазинчик': 384,\n",
       " 'блестк': 385,\n",
       " 'домосед': 386,\n",
       " 'возможн': 387,\n",
       " 'очк': 388,\n",
       " 'фонтан': 389,\n",
       " 'информатик': 390,\n",
       " 'ве': 391,\n",
       " 'фигур': 392,\n",
       " 'средн': 393,\n",
       " 'девушк': 394,\n",
       " 'будущ': 395,\n",
       " 'неб': 396,\n",
       " 'умн': 397,\n",
       " 'звезд': 398,\n",
       " 'полн': 399,\n",
       " 'котик': 400,\n",
       " 'дым': 401,\n",
       " 'пита': 402,\n",
       " 'знающ': 403,\n",
       " 'поддержа': 404,\n",
       " 'комфорт': 405,\n",
       " 'слож': 406,\n",
       " 'времяпрепровожден': 407,\n",
       " 'забавн': 408,\n",
       " 'чтонибуд': 409,\n",
       " 'напиш': 410,\n",
       " 'вальс': 411,\n",
       " 'тематик': 412,\n",
       " 'доверя': 413,\n",
       " 'провож': 414,\n",
       " 'сноуборд': 415,\n",
       " 'темн': 416,\n",
       " 'томат': 417,\n",
       " 'вялен': 418,\n",
       " 'сумк': 419,\n",
       " 'велик': 420,\n",
       " 'ката': 421,\n",
       " 'прям': 422,\n",
       " 'сумерк': 423,\n",
       " 'выйт': 424,\n",
       " 'вечер': 425,\n",
       " 'чемнибуд': 426,\n",
       " 'тикток': 427,\n",
       " 'ютуб': 428,\n",
       " 'зависа': 429,\n",
       " 'орех': 430,\n",
       " 'официантк': 431,\n",
       " 'привет': 432,\n",
       " 'капучин': 433,\n",
       " 'говн': 434,\n",
       " 'рэп': 435,\n",
       " 'калья': 436,\n",
       " 'разн': 437,\n",
       " 'математическ': 438,\n",
       " 'провод': 439,\n",
       " 'поездок': 440,\n",
       " 'ответствен': 441,\n",
       " 'настолк': 442,\n",
       " 'парен': 443,\n",
       " 'светл': 444,\n",
       " 'молод': 445,\n",
       " 'серьезн': 446,\n",
       " 'порядочн': 447,\n",
       " 'частеньк': 448,\n",
       " 'виде': 449,\n",
       " 'разнообраз': 450,\n",
       " 'невыносим': 451,\n",
       " 'привычк': 452,\n",
       " 'абсолютн': 453,\n",
       " 'грамотн': 454,\n",
       " 'повод': 455,\n",
       " 'подушн': 456,\n",
       " 'свет': 457,\n",
       " 'периодическ': 458,\n",
       " 'адаптирован': 459,\n",
       " 'инач': 460,\n",
       " 'какт': 461,\n",
       " 'сход': 462,\n",
       " 'проч': 463,\n",
       " 'цел': 464,\n",
       " 'мо': 465,\n",
       " 'социальн': 466,\n",
       " 'ребенок': 467,\n",
       " 'стат': 468,\n",
       " 'медитац': 469,\n",
       " 'историческ': 470,\n",
       " 'е': 471,\n",
       " 'мистическ': 472,\n",
       " 'крайм': 473,\n",
       " 'тру': 474,\n",
       " 'йог': 475,\n",
       " 'оцен': 476,\n",
       " 'понима': 477,\n",
       " 'пивк': 478,\n",
       " 'бар': 479,\n",
       " 'повзрослел': 480,\n",
       " 'быстр': 481,\n",
       " 'вокруг': 482,\n",
       " 'поч': 483,\n",
       " 'меломанк': 484,\n",
       " 'творчеств': 485,\n",
       " 'азиат': 486,\n",
       " 'раньш': 487,\n",
       " 'истеричк': 488,\n",
       " 'егэ': 489,\n",
       " 'правд': 490,\n",
       " 'химб': 491,\n",
       " 'эзотерик': 492,\n",
       " 'интересова': 493,\n",
       " 'пх': 494,\n",
       " 'мя': 495,\n",
       " 'херн': 496,\n",
       " 'ненужн': 497,\n",
       " 'куч': 498,\n",
       " 'скупа': 499,\n",
       " 'визаж': 500,\n",
       " 'друж': 501,\n",
       " 'реальн': 502,\n",
       " 'авик': 503,\n",
       " 'составля': 504,\n",
       " 'панк': 505,\n",
       " 'удаля': 506,\n",
       " 'пж': 507,\n",
       " 'дорис': 508,\n",
       " 'какаят': 509,\n",
       " 'параш': 510,\n",
       " 'головн': 511,\n",
       " 'пост': 512,\n",
       " 'бебеб': 513,\n",
       " 'генш': 514,\n",
       " 'сиж': 515,\n",
       " 'посиделочк': 516,\n",
       " 'кофеек': 517,\n",
       " 'чаек': 518,\n",
       " 'шашк': 519,\n",
       " 'стеснительн': 520,\n",
       " 'курс': 521,\n",
       " 'верн': 522,\n",
       " 'подробн': 523,\n",
       " 'ко': 524,\n",
       " 'тоб': 525,\n",
       " 'ов': 526,\n",
       " 'лях': 527,\n",
       " 'карамельн': 528,\n",
       " 'чат': 529,\n",
       " 'улыбк': 530,\n",
       " 'голубоглаз': 531,\n",
       " 'посмотрет': 532,\n",
       " 'недавн': 533,\n",
       " 'привк': 534,\n",
       " 'тем': 535,\n",
       " 'прикол': 536,\n",
       " 'возьм': 537,\n",
       " 'агентств': 538,\n",
       " 'брюнетк': 539,\n",
       " 'втор': 540,\n",
       " 'отнош': 541,\n",
       " 'семнадца': 542,\n",
       " 'меркантильн': 543,\n",
       " 'пм': 544,\n",
       " 'мер': 545,\n",
       " 'алкогол': 546,\n",
       " 'курен': 547,\n",
       " 'спокойн': 548,\n",
       " 'дружелюбн': 549,\n",
       " 'спиц': 550,\n",
       " 'вяж': 551,\n",
       " 'подкаст': 552,\n",
       " 'психолог': 553,\n",
       " 'программист': 554,\n",
       " 'сфер': 555,\n",
       " 'дв': 556,\n",
       " 'жела': 557,\n",
       " 'личн': 558,\n",
       " 'одежд': 559,\n",
       " 'прикиньт': 560,\n",
       " 'ночн': 561,\n",
       " 'покуша': 562,\n",
       " 'ясн': 563,\n",
       " 'коротк': 564,\n",
       " 'врод': 565,\n",
       " 'посматрива': 566,\n",
       " 'разбира': 567,\n",
       " 'поболта': 568,\n",
       " 'альбед': 569,\n",
       " 'эм': 570,\n",
       " 'плава': 571,\n",
       " 'резюм': 572,\n",
       " 'кратк': 573,\n",
       " 'пет': 574,\n",
       " 'алгебр': 575,\n",
       " 'ид': 576,\n",
       " 'удаст': 577,\n",
       " 'приготовл': 578,\n",
       " 'близк': 579,\n",
       " 'стан': 580,\n",
       " 'капибар': 581,\n",
       " 'адекват': 582,\n",
       " 'наслажда': 583,\n",
       " 'над': 584,\n",
       " 'забрел': 585,\n",
       " 'чащ': 586,\n",
       " 'прогулк': 587,\n",
       " 'позитивн': 588,\n",
       " 'активн': 589,\n",
       " 'объят': 590,\n",
       " 'хобб': 591,\n",
       " 'пен': 592,\n",
       " 'закрыт': 593,\n",
       " 'вечерк': 594,\n",
       " 'музыкант': 595,\n",
       " 'солнышк': 596,\n",
       " 'чист': 597,\n",
       " 'сантиметр': 598,\n",
       " 'нан': 599,\n",
       " 'кам': 600,\n",
       " 'кафк': 601,\n",
       " 'показа': 602,\n",
       " 'достойн': 603,\n",
       " 'пхпх': 604,\n",
       " 'социализирова': 605,\n",
       " 'попытк': 606,\n",
       " 'слушател': 607,\n",
       " 'ха': 608,\n",
       " 'сегодн': 609,\n",
       " 'любв': 610,\n",
       " 'мат': 611,\n",
       " 'эээ': 612,\n",
       " 'общ': 613,\n",
       " 'очеред': 614,\n",
       " 'кабанчик': 615,\n",
       " 'собствен': 616,\n",
       " 'хоррор': 617,\n",
       " 'пойт': 618,\n",
       " 'адекватн': 619,\n",
       " 'пойд': 620,\n",
       " 'бмв': 621,\n",
       " 'скрыва': 622,\n",
       " 'довольнотак': 623,\n",
       " 'угодн': 624,\n",
       " 'все': 625,\n",
       " 'обменива': 626,\n",
       " 'м': 627,\n",
       " 'модельн': 628}"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w2v_model = Word2Vec(min_count = 1, window=5, vector_size=300)\n",
    "w2v_model.build_vocab(train_text)\n",
    "w2v_model.train(train_text, total_examples=w2v_model.corpus_count, epochs=10, report_delay=1)\n",
    "w2v_model.wv.key_to_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "e2e5324c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dtype('float32')"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w2v_model.wv[['хим', 'дел']].dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "01f8ce50",
   "metadata": {},
   "outputs": [],
   "source": [
    "#ДЛЯ word2vec EMBEDDINGS\n",
    "class DataPreprocessing(torch.utils.data.Dataset):\n",
    "    def __init__(self, df, subset='train'):\n",
    "        self.subset = subset\n",
    "        self.length_of_vec = 32\n",
    "        #self.x_train, self.x_test, self.y_train, self.y_test = model_selection.train_test_split(self.df['Описание анкеты'], self.df[' Рекомендовать'], test_size=0.33, random_state=42)\n",
    "        df_t = df[df[' Рекомендовать'] == 1].iloc[0:16]\n",
    "        df_f = df[df[' Рекомендовать'] == 0].iloc[0:48]\n",
    "        df_test = pd.concat([df_t, df_f], ignore_index=True)\n",
    "        self.x_test = df_test['Описание анкеты']\n",
    "        self.y_test = df_test[' Рекомендовать']\n",
    "        train = df.drop(index=df_t.index.append(df_f.index))\n",
    "        self.x_train = train['Описание анкеты']\n",
    "        self.y_train = train[' Рекомендовать']\n",
    "        \n",
    "    def vectorize(self, disc):\n",
    "        tokens = disc.split(' ')\n",
    "        embedded = w2v_model.wv[tokens]\n",
    "        if embedded.shape[0] < self.length_of_vec:\n",
    "            addition = np.zeros((self.length_of_vec-embedded.shape[0], 300), dtype=np.float32)\n",
    "            embedded = np.vstack((embedded, addition))\n",
    "        return torch.tensor(embedded, device=device)\n",
    "                \n",
    "    def __len__(self):\n",
    "        if self.subset == 'train':\n",
    "            return len(self.x_train)\n",
    "        else:\n",
    "            return len(self.x_test)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        if self.subset == 'train':\n",
    "            disc = self.x_train.values[index]\n",
    "            recomend = self.y_train.values[index].astype(float)\n",
    "        else:\n",
    "            disc = self.x_test.values[index]\n",
    "            recomend = self.y_test.values[index].astype(float)\n",
    "        if type(disc) is str:\n",
    "            return self.vectorize(disc), torch.tensor(recomend, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c1c2f470",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Описание анкеты</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Рекомендовать</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>148</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>48</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                Описание анкеты\n",
       " Рекомендовать                 \n",
       "0                           148\n",
       "1                            48"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_data.groupby(by=' Рекомендовать').count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a26bd8c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True: 16\n",
      "False: 148\n"
     ]
    }
   ],
   "source": [
    "print('True: 16')\n",
    "print('False: 148')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "98b751d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#ДЛЯ СВОИХ EMBEDDINGS\n",
    "class DataPreprocessing_self_emb(torch.utils.data.Dataset):\n",
    "    def __init__(self, df, subset='train'):\n",
    "        self.subset = subset\n",
    "        self.length_of_vec = 32\n",
    "        #self.x_train, self.x_test, self.y_train, self.y_test = model_selection.train_test_split(self.df['Описание анкеты'], self.df[' Рекомендовать'], test_size=0.33, random_state=42)\n",
    "        df_t = df[df[' Рекомендовать'] == 1].iloc[0:16]\n",
    "        df_f = df[df[' Рекомендовать'] == 0].iloc[0:48]\n",
    "        df_test = pd.concat([df_t, df_f], ignore_index=True)\n",
    "        self.x_test = df_test['Описание анкеты']\n",
    "        self.y_test = df_test[' Рекомендовать']\n",
    "        train = df.drop(index=df_t.index.append(df_f.index))\n",
    "        self.x_train = train['Описание анкеты']\n",
    "        self.y_train = train[' Рекомендовать']\n",
    "        \n",
    "    def vectorize(self, disc):\n",
    "        tokens = disc.split(' ')\n",
    "        ids = []\n",
    "        for j in tokens:\n",
    "            if j in dictionary:\n",
    "                ids.append(dictionary[j])\n",
    "            else:\n",
    "                ids.append(dictionary['<unk>'])\n",
    "        if len(ids) < self.length_of_vec:\n",
    "            addition = [0 for i in range(self.length_of_vec - len(ids))]\n",
    "            ids += addition\n",
    "        return torch.tensor(ids, device=device) \n",
    "                \n",
    "    def __len__(self):\n",
    "        if self.subset == 'train':\n",
    "            return len(self.x_train)\n",
    "        else:\n",
    "            return len(self.x_test)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        if self.subset == 'train':\n",
    "            disc = self.x_train.values[index]\n",
    "            recomend = self.y_train.values[index].astype(float)\n",
    "        else:\n",
    "            disc = self.x_test.values[index]\n",
    "            recomend = self.y_test.values[index].astype(float)\n",
    "        if type(disc) is str:\n",
    "            return self.vectorize(disc), torch.tensor(recomend, device=device) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "2b79610c",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = DataPreprocessing_self_emb(df_data)\n",
    "data_w2v = DataPreprocessing(df_data)\n",
    "batch_size = len(data)\n",
    "epochs = 150"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a98ef4b",
   "metadata": {},
   "source": [
    "Input matrix 44x300"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "00e689e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.emb = torch.nn.Embedding(len(dictionary), 300)\n",
    "        self.rnn = torch.nn.LSTM(300, 300, 32, batch_first=True)\n",
    "        #self.sequential = torch.nn.Sequential(\n",
    "            #torch.nn.Conv1d(self.dim, self.dim, 3),\n",
    "            #torch.nn.Conv1d(self.dim, self.dim, 3),\n",
    "            #torch.nn.ELU(),\n",
    "            #torch.nn.MaxPool1d(2),\n",
    "            #\n",
    "            #torch.nn.Conv1d(self.dim, self.dim, 3),\n",
    "            #torch.nn.Conv1d(self.dim, self.dim, 3),\n",
    "            #torch.nn.ELU(),\n",
    "            #torch.nn.MaxPool1d(2),\n",
    "            #\n",
    "            #torch.nn.Conv1d(self.dim, self.dim, 3),\n",
    "            #torch.nn.Conv1d(self.dim, self.dim, 3),\n",
    "            #torch.nn.ELU(),\n",
    "            #torch.nn.MaxPool1d(2),\n",
    "            #\n",
    "            #torch.nn.Conv1d(self.dim, self.dim, 3),\n",
    "            #torch.nn.Conv1d(self.dim, self.dim, 3),\n",
    "            #torch.nn.ELU(),\n",
    "            #torch.nn.MaxPool1d(2),\n",
    "        #)\n",
    "        self.flat = torch.nn.Flatten()\n",
    "        self.lin1 = torch.nn.Linear(9600, 100)\n",
    "        self.lin2 = torch.nn.Linear(100, 1)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        embedded_x = self.emb(x)\n",
    "        y, (hidden, cell) = self.rnn(embedded_x)\n",
    "        #y = self.sequential(embedded_x)\n",
    "        y = self.flat(y)\n",
    "        y = torch.nn.functional.tanh(self.lin1(y))\n",
    "        y = torch.sigmoid(self.lin2(y))\n",
    "        return y\n",
    "    \n",
    "    def fit(self, data, batch_size, epochs):\n",
    "        dl = torch.utils.data.DataLoader(data, batch_size=batch_size)\n",
    "        loss_fn = torch.nn.BCELoss()\n",
    "        optimizer = torch.optim.Adam(self.parameters(), 0.01, weight_decay=0.1)\n",
    "        self.train()\n",
    "        for epoch in range(epochs):\n",
    "            Loss = 0\n",
    "            print(f'-------- epoch = {epoch+1}/{epochs} --------')\n",
    "            for batch, (X, Y) in enumerate(dl):\n",
    "                optimizer.zero_grad()\n",
    "                pred = self(X).to(device)###\n",
    "                target = torch.reshape(Y.to(device), (-1, 1)).float()\n",
    "                loss = loss_fn(pred, target)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                #if batch == int(len(data)/batch_size):\n",
    "                    #loss = loss.item()\n",
    "                    #print(f'batch = {batch}; loss = {loss}')\n",
    "                if batch % 1 == 0:\n",
    "                    loss, current = loss.item(), (batch + 1)*len(X)\n",
    "                    Loss += loss\n",
    "                    print(f'batch: {batch+1}; loss = {loss}')\n",
    "            print(f'-------- loss = {Loss/(int(len(data)/batch_size))} --------')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "99af8145",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model_w2v(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.rnn = torch.nn.LSTM(300, 300, 32, batch_first=True)\n",
    "        self.flat = torch.nn.Flatten()\n",
    "        self.lin1 = torch.nn.Linear(9600, 100)\n",
    "        self.lin2 = torch.nn.Linear(100, 1)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        y, (hidden, cell) = self.rnn(x)\n",
    "        y = self.flat(y)\n",
    "        y = torch.nn.functional.tanh(self.lin1(y))\n",
    "        y = torch.sigmoid(self.lin2(y))\n",
    "        return y\n",
    "    \n",
    "    def fit(self, data, batch_size, epochs):\n",
    "        dl = torch.utils.data.DataLoader(data, batch_size=batch_size)\n",
    "        loss_fn = torch.nn.BCELoss()\n",
    "        optimizer = torch.optim.Adam(self.parameters(), weight_decay=0.05)\n",
    "        self.train()\n",
    "        for epoch in range(epochs):\n",
    "            Loss = 0\n",
    "            print(f'-------- epoch = {epoch+1}/{epochs} --------')\n",
    "            for batch, (X, Y) in enumerate(dl):\n",
    "                optimizer.zero_grad()\n",
    "                pred = self(X).to(device)###\n",
    "                target = torch.reshape(Y.to(device), (-1, 1)).float()\n",
    "                loss = loss_fn(pred, target)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                if batch % 1 == 0:\n",
    "                    loss, current = loss.item(), (batch + 1)*len(X)\n",
    "                    Loss += loss\n",
    "                    print(f'batch: {batch+1}; loss = {loss}')\n",
    "            print(f'-------- loss = {Loss/(int(len(data)/batch_size))} --------')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "9cb2aefe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Model_w2v(\n",
       "  (rnn): LSTM(300, 300, num_layers=32, batch_first=True)\n",
       "  (flat): Flatten(start_dim=1, end_dim=-1)\n",
       "  (lin1): Linear(in_features=9600, out_features=100, bias=True)\n",
       "  (lin2): Linear(in_features=100, out_features=1, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = Model()\n",
    "model_w2v = Model_w2v()\n",
    "model.to(device)\n",
    "model_w2v.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "572edb84",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------- epoch = 1/150 --------\n",
      "batch: 1; loss = 0.6834415197372437\n",
      "-------- loss = 0.6834415197372437 --------\n",
      "-------- epoch = 2/150 --------\n",
      "batch: 1; loss = 0.5735373497009277\n",
      "-------- loss = 0.5735373497009277 --------\n",
      "-------- epoch = 3/150 --------\n",
      "batch: 1; loss = 0.5901252031326294\n",
      "-------- loss = 0.5901252031326294 --------\n",
      "-------- epoch = 4/150 --------\n",
      "batch: 1; loss = 0.6189671158790588\n",
      "-------- loss = 0.6189671158790588 --------\n",
      "-------- epoch = 5/150 --------\n",
      "batch: 1; loss = 0.6494576334953308\n",
      "-------- loss = 0.6494576334953308 --------\n",
      "-------- epoch = 6/150 --------\n",
      "batch: 1; loss = 0.6658746004104614\n",
      "-------- loss = 0.6658746004104614 --------\n",
      "-------- epoch = 7/150 --------\n",
      "batch: 1; loss = 0.6660979390144348\n",
      "-------- loss = 0.6660979390144348 --------\n",
      "-------- epoch = 8/150 --------\n",
      "batch: 1; loss = 0.6603807806968689\n",
      "-------- loss = 0.6603807806968689 --------\n",
      "-------- epoch = 9/150 --------\n",
      "batch: 1; loss = 0.6583582162857056\n",
      "-------- loss = 0.6583582162857056 --------\n",
      "-------- epoch = 10/150 --------\n",
      "batch: 1; loss = 0.6591340899467468\n",
      "-------- loss = 0.6591340899467468 --------\n",
      "-------- epoch = 11/150 --------\n",
      "batch: 1; loss = 0.6581385135650635\n",
      "-------- loss = 0.6581385135650635 --------\n",
      "-------- epoch = 12/150 --------\n",
      "batch: 1; loss = 0.6542396545410156\n",
      "-------- loss = 0.6542396545410156 --------\n",
      "-------- epoch = 13/150 --------\n",
      "batch: 1; loss = 0.6494182348251343\n",
      "-------- loss = 0.6494182348251343 --------\n",
      "-------- epoch = 14/150 --------\n",
      "batch: 1; loss = 0.6466048359870911\n",
      "-------- loss = 0.6466048359870911 --------\n",
      "-------- epoch = 15/150 --------\n",
      "batch: 1; loss = 0.6468790173530579\n",
      "-------- loss = 0.6468790173530579 --------\n",
      "-------- epoch = 16/150 --------\n",
      "batch: 1; loss = 0.6482084393501282\n",
      "-------- loss = 0.6482084393501282 --------\n",
      "-------- epoch = 17/150 --------\n",
      "batch: 1; loss = 0.6473632454872131\n",
      "-------- loss = 0.6473632454872131 --------\n",
      "-------- epoch = 18/150 --------\n",
      "batch: 1; loss = 0.643650472164154\n",
      "-------- loss = 0.643650472164154 --------\n",
      "-------- epoch = 19/150 --------\n",
      "batch: 1; loss = 0.6395982503890991\n",
      "-------- loss = 0.6395982503890991 --------\n",
      "-------- epoch = 20/150 --------\n",
      "batch: 1; loss = 0.6374885439872742\n",
      "-------- loss = 0.6374885439872742 --------\n",
      "-------- epoch = 21/150 --------\n",
      "batch: 1; loss = 0.6371304988861084\n",
      "-------- loss = 0.6371304988861084 --------\n",
      "-------- epoch = 22/150 --------\n",
      "batch: 1; loss = 0.6368737816810608\n",
      "-------- loss = 0.6368737816810608 --------\n",
      "-------- epoch = 23/150 --------\n",
      "batch: 1; loss = 0.635676920413971\n",
      "-------- loss = 0.635676920413971 --------\n",
      "-------- epoch = 24/150 --------\n",
      "batch: 1; loss = 0.633886992931366\n",
      "-------- loss = 0.633886992931366 --------\n",
      "-------- epoch = 25/150 --------\n",
      "batch: 1; loss = 0.6322962641716003\n",
      "-------- loss = 0.6322962641716003 --------\n",
      "-------- epoch = 26/150 --------\n",
      "batch: 1; loss = 0.6311607360839844\n",
      "-------- loss = 0.6311607360839844 --------\n",
      "-------- epoch = 27/150 --------\n",
      "batch: 1; loss = 0.6302290558815002\n",
      "-------- loss = 0.6302290558815002 --------\n",
      "-------- epoch = 28/150 --------\n",
      "batch: 1; loss = 0.6292404532432556\n",
      "-------- loss = 0.6292404532432556 --------\n",
      "-------- epoch = 29/150 --------\n",
      "batch: 1; loss = 0.6281591653823853\n",
      "-------- loss = 0.6281591653823853 --------\n",
      "-------- epoch = 30/150 --------\n",
      "batch: 1; loss = 0.6270670890808105\n",
      "-------- loss = 0.6270670890808105 --------\n",
      "-------- epoch = 31/150 --------\n",
      "batch: 1; loss = 0.6260045170783997\n",
      "-------- loss = 0.6260045170783997 --------\n",
      "-------- epoch = 32/150 --------\n",
      "batch: 1; loss = 0.6249479055404663\n",
      "-------- loss = 0.6249479055404663 --------\n",
      "-------- epoch = 33/150 --------\n",
      "batch: 1; loss = 0.6238890886306763\n",
      "-------- loss = 0.6238890886306763 --------\n",
      "-------- epoch = 34/150 --------\n",
      "batch: 1; loss = 0.6228747963905334\n",
      "-------- loss = 0.6228747963905334 --------\n",
      "-------- epoch = 35/150 --------\n",
      "batch: 1; loss = 0.6219616532325745\n",
      "-------- loss = 0.6219616532325745 --------\n",
      "-------- epoch = 36/150 --------\n",
      "batch: 1; loss = 0.6211658120155334\n",
      "-------- loss = 0.6211658120155334 --------\n",
      "-------- epoch = 37/150 --------\n",
      "batch: 1; loss = 0.6204631924629211\n",
      "-------- loss = 0.6204631924629211 --------\n",
      "-------- epoch = 38/150 --------\n",
      "batch: 1; loss = 0.6198235154151917\n",
      "-------- loss = 0.6198235154151917 --------\n",
      "-------- epoch = 39/150 --------\n",
      "batch: 1; loss = 0.6192349791526794\n",
      "-------- loss = 0.6192349791526794 --------\n",
      "-------- epoch = 40/150 --------\n",
      "batch: 1; loss = 0.6187030076980591\n",
      "-------- loss = 0.6187030076980591 --------\n",
      "-------- epoch = 41/150 --------\n",
      "batch: 1; loss = 0.6182311177253723\n",
      "-------- loss = 0.6182311177253723 --------\n",
      "-------- epoch = 42/150 --------\n",
      "batch: 1; loss = 0.6178118586540222\n",
      "-------- loss = 0.6178118586540222 --------\n",
      "-------- epoch = 43/150 --------\n",
      "batch: 1; loss = 0.6174344420433044\n",
      "-------- loss = 0.6174344420433044 --------\n",
      "-------- epoch = 44/150 --------\n",
      "batch: 1; loss = 0.6170958280563354\n",
      "-------- loss = 0.6170958280563354 --------\n",
      "-------- epoch = 45/150 --------\n",
      "batch: 1; loss = 0.6167987585067749\n",
      "-------- loss = 0.6167987585067749 --------\n",
      "-------- epoch = 46/150 --------\n",
      "batch: 1; loss = 0.6165443062782288\n",
      "-------- loss = 0.6165443062782288 --------\n",
      "-------- epoch = 47/150 --------\n",
      "batch: 1; loss = 0.6163296103477478\n",
      "-------- loss = 0.6163296103477478 --------\n",
      "-------- epoch = 48/150 --------\n",
      "batch: 1; loss = 0.6161509156227112\n",
      "-------- loss = 0.6161509156227112 --------\n",
      "-------- epoch = 49/150 --------\n",
      "batch: 1; loss = 0.6160066723823547\n",
      "-------- loss = 0.6160066723823547 --------\n",
      "-------- epoch = 50/150 --------\n",
      "batch: 1; loss = 0.6158968806266785\n",
      "-------- loss = 0.6158968806266785 --------\n",
      "-------- epoch = 51/150 --------\n",
      "batch: 1; loss = 0.615821123123169\n",
      "-------- loss = 0.615821123123169 --------\n",
      "-------- epoch = 52/150 --------\n",
      "batch: 1; loss = 0.615777850151062\n",
      "-------- loss = 0.615777850151062 --------\n",
      "-------- epoch = 53/150 --------\n",
      "batch: 1; loss = 0.6157659888267517\n",
      "-------- loss = 0.6157659888267517 --------\n",
      "-------- epoch = 54/150 --------\n",
      "batch: 1; loss = 0.6157839298248291\n",
      "-------- loss = 0.6157839298248291 --------\n",
      "-------- epoch = 55/150 --------\n",
      "batch: 1; loss = 0.6158301830291748\n",
      "-------- loss = 0.6158301830291748 --------\n",
      "-------- epoch = 56/150 --------\n",
      "batch: 1; loss = 0.6159021258354187\n",
      "-------- loss = 0.6159021258354187 --------\n",
      "-------- epoch = 57/150 --------\n",
      "batch: 1; loss = 0.6159969568252563\n",
      "-------- loss = 0.6159969568252563 --------\n",
      "-------- epoch = 58/150 --------\n",
      "batch: 1; loss = 0.6161108016967773\n",
      "-------- loss = 0.6161108016967773 --------\n",
      "-------- epoch = 59/150 --------\n",
      "batch: 1; loss = 0.616239607334137\n",
      "-------- loss = 0.616239607334137 --------\n",
      "-------- epoch = 60/150 --------\n",
      "batch: 1; loss = 0.6163803339004517\n",
      "-------- loss = 0.6163803339004517 --------\n",
      "-------- epoch = 61/150 --------\n",
      "batch: 1; loss = 0.6165304183959961\n",
      "-------- loss = 0.6165304183959961 --------\n",
      "-------- epoch = 62/150 --------\n",
      "batch: 1; loss = 0.6166878938674927\n",
      "-------- loss = 0.6166878938674927 --------\n",
      "-------- epoch = 63/150 --------\n",
      "batch: 1; loss = 0.6168510913848877\n",
      "-------- loss = 0.6168510913848877 --------\n",
      "-------- epoch = 64/150 --------\n",
      "batch: 1; loss = 0.6170188188552856\n",
      "-------- loss = 0.6170188188552856 --------\n",
      "-------- epoch = 65/150 --------\n",
      "batch: 1; loss = 0.6171900033950806\n",
      "-------- loss = 0.6171900033950806 --------\n",
      "-------- epoch = 66/150 --------\n",
      "batch: 1; loss = 0.6173632144927979\n",
      "-------- loss = 0.6173632144927979 --------\n",
      "-------- epoch = 67/150 --------\n",
      "batch: 1; loss = 0.6175373196601868\n",
      "-------- loss = 0.6175373196601868 --------\n",
      "-------- epoch = 68/150 --------\n",
      "batch: 1; loss = 0.6177108883857727\n",
      "-------- loss = 0.6177108883857727 --------\n",
      "-------- epoch = 69/150 --------\n",
      "batch: 1; loss = 0.6178823113441467\n",
      "-------- loss = 0.6178823113441467 --------\n",
      "-------- epoch = 70/150 --------\n",
      "batch: 1; loss = 0.6180503368377686\n",
      "-------- loss = 0.6180503368377686 --------\n",
      "-------- epoch = 71/150 --------\n",
      "batch: 1; loss = 0.6182133555412292\n",
      "-------- loss = 0.6182133555412292 --------\n",
      "-------- epoch = 72/150 --------\n",
      "batch: 1; loss = 0.6183701157569885\n",
      "-------- loss = 0.6183701157569885 --------\n",
      "-------- epoch = 73/150 --------\n",
      "batch: 1; loss = 0.6185195446014404\n",
      "-------- loss = 0.6185195446014404 --------\n",
      "-------- epoch = 74/150 --------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch: 1; loss = 0.6186608076095581\n",
      "-------- loss = 0.6186608076095581 --------\n",
      "-------- epoch = 75/150 --------\n",
      "batch: 1; loss = 0.6187936067581177\n",
      "-------- loss = 0.6187936067581177 --------\n",
      "-------- epoch = 76/150 --------\n",
      "batch: 1; loss = 0.6189175844192505\n",
      "-------- loss = 0.6189175844192505 --------\n",
      "-------- epoch = 77/150 --------\n",
      "batch: 1; loss = 0.6190324425697327\n",
      "-------- loss = 0.6190324425697327 --------\n",
      "-------- epoch = 78/150 --------\n",
      "batch: 1; loss = 0.6191380023956299\n",
      "-------- loss = 0.6191380023956299 --------\n",
      "-------- epoch = 79/150 --------\n",
      "batch: 1; loss = 0.619234025478363\n",
      "-------- loss = 0.619234025478363 --------\n",
      "-------- epoch = 80/150 --------\n",
      "batch: 1; loss = 0.6193203926086426\n",
      "-------- loss = 0.6193203926086426 --------\n",
      "-------- epoch = 81/150 --------\n",
      "batch: 1; loss = 0.619396984577179\n",
      "-------- loss = 0.619396984577179 --------\n",
      "-------- epoch = 82/150 --------\n",
      "batch: 1; loss = 0.6194639205932617\n",
      "-------- loss = 0.6194639205932617 --------\n",
      "-------- epoch = 83/150 --------\n",
      "batch: 1; loss = 0.6195216774940491\n",
      "-------- loss = 0.6195216774940491 --------\n",
      "-------- epoch = 84/150 --------\n",
      "batch: 1; loss = 0.6195703148841858\n",
      "-------- loss = 0.6195703148841858 --------\n",
      "-------- epoch = 85/150 --------\n",
      "batch: 1; loss = 0.619610071182251\n",
      "-------- loss = 0.619610071182251 --------\n",
      "-------- epoch = 86/150 --------\n",
      "batch: 1; loss = 0.6196416616439819\n",
      "-------- loss = 0.6196416616439819 --------\n",
      "-------- epoch = 87/150 --------\n",
      "batch: 1; loss = 0.6196653246879578\n",
      "-------- loss = 0.6196653246879578 --------\n",
      "-------- epoch = 88/150 --------\n",
      "batch: 1; loss = 0.6196815371513367\n",
      "-------- loss = 0.6196815371513367 --------\n",
      "-------- epoch = 89/150 --------\n",
      "batch: 1; loss = 0.6196906566619873\n",
      "-------- loss = 0.6196906566619873 --------\n",
      "-------- epoch = 90/150 --------\n",
      "batch: 1; loss = 0.6196932792663574\n",
      "-------- loss = 0.6196932792663574 --------\n",
      "-------- epoch = 91/150 --------\n",
      "batch: 1; loss = 0.6196898818016052\n",
      "-------- loss = 0.6196898818016052 --------\n",
      "-------- epoch = 92/150 --------\n",
      "batch: 1; loss = 0.6196811199188232\n",
      "-------- loss = 0.6196811199188232 --------\n",
      "-------- epoch = 93/150 --------\n",
      "batch: 1; loss = 0.6196675896644592\n",
      "-------- loss = 0.6196675896644592 --------\n",
      "-------- epoch = 94/150 --------\n",
      "batch: 1; loss = 0.6196499466896057\n",
      "-------- loss = 0.6196499466896057 --------\n",
      "-------- epoch = 95/150 --------\n",
      "batch: 1; loss = 0.6196285486221313\n",
      "-------- loss = 0.6196285486221313 --------\n",
      "-------- epoch = 96/150 --------\n",
      "batch: 1; loss = 0.6196042895317078\n",
      "-------- loss = 0.6196042895317078 --------\n",
      "-------- epoch = 97/150 --------\n",
      "batch: 1; loss = 0.6195774674415588\n",
      "-------- loss = 0.6195774674415588 --------\n",
      "-------- epoch = 98/150 --------\n",
      "batch: 1; loss = 0.6195485591888428\n",
      "-------- loss = 0.6195485591888428 --------\n",
      "-------- epoch = 99/150 --------\n",
      "batch: 1; loss = 0.6195181608200073\n",
      "-------- loss = 0.6195181608200073 --------\n",
      "-------- epoch = 100/150 --------\n",
      "batch: 1; loss = 0.6194866299629211\n",
      "-------- loss = 0.6194866299629211 --------\n",
      "-------- epoch = 101/150 --------\n",
      "batch: 1; loss = 0.6194545030593872\n",
      "-------- loss = 0.6194545030593872 --------\n",
      "-------- epoch = 102/150 --------\n",
      "batch: 1; loss = 0.6194220781326294\n",
      "-------- loss = 0.6194220781326294 --------\n",
      "-------- epoch = 103/150 --------\n",
      "batch: 1; loss = 0.6193898320198059\n",
      "-------- loss = 0.6193898320198059 --------\n",
      "-------- epoch = 104/150 --------\n",
      "batch: 1; loss = 0.6193580627441406\n",
      "-------- loss = 0.6193580627441406 --------\n",
      "-------- epoch = 105/150 --------\n",
      "batch: 1; loss = 0.619327187538147\n",
      "-------- loss = 0.619327187538147 --------\n",
      "-------- epoch = 106/150 --------\n",
      "batch: 1; loss = 0.6192972660064697\n",
      "-------- loss = 0.6192972660064697 --------\n",
      "-------- epoch = 107/150 --------\n",
      "batch: 1; loss = 0.6192688345909119\n",
      "-------- loss = 0.6192688345909119 --------\n",
      "-------- epoch = 108/150 --------\n",
      "batch: 1; loss = 0.6192418932914734\n",
      "-------- loss = 0.6192418932914734 --------\n",
      "-------- epoch = 109/150 --------\n",
      "batch: 1; loss = 0.6192165613174438\n",
      "-------- loss = 0.6192165613174438 --------\n",
      "-------- epoch = 110/150 --------\n",
      "batch: 1; loss = 0.6191930174827576\n",
      "-------- loss = 0.6191930174827576 --------\n",
      "-------- epoch = 111/150 --------\n",
      "batch: 1; loss = 0.6191714406013489\n",
      "-------- loss = 0.6191714406013489 --------\n",
      "-------- epoch = 112/150 --------\n",
      "batch: 1; loss = 0.619151771068573\n",
      "-------- loss = 0.619151771068573 --------\n",
      "-------- epoch = 113/150 --------\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn [35], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn [33], line 53\u001b[0m, in \u001b[0;36mModel.fit\u001b[1;34m(self, data, batch_size, epochs)\u001b[0m\n\u001b[0;32m     51\u001b[0m target \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mreshape(Y\u001b[38;5;241m.\u001b[39mto(device), (\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m1\u001b[39m))\u001b[38;5;241m.\u001b[39mfloat()\n\u001b[0;32m     52\u001b[0m loss \u001b[38;5;241m=\u001b[39m loss_fn(pred, target)\n\u001b[1;32m---> 53\u001b[0m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     54\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[0;32m     55\u001b[0m \u001b[38;5;66;03m#if batch == int(len(data)/batch_size):\u001b[39;00m\n\u001b[0;32m     56\u001b[0m     \u001b[38;5;66;03m#loss = loss.item()\u001b[39;00m\n\u001b[0;32m     57\u001b[0m     \u001b[38;5;66;03m#print(f'batch = {batch}; loss = {loss}')\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\_tensor.py:487\u001b[0m, in \u001b[0;36mTensor.backward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    477\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    478\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[0;32m    479\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[0;32m    480\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    485\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[0;32m    486\u001b[0m     )\n\u001b[1;32m--> 487\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    488\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[0;32m    489\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\autograd\\__init__.py:200\u001b[0m, in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    195\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[0;32m    197\u001b[0m \u001b[38;5;66;03m# The reason we repeat same the comment below is that\u001b[39;00m\n\u001b[0;32m    198\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[0;32m    199\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[1;32m--> 200\u001b[0m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[0;32m    201\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    202\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model.fit(data, batch_size, epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "577f0c1c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------- epoch = 1/150 --------\n",
      "batch: 1; loss = 0.6827437877655029\n",
      "-------- loss = 0.6827437877655029 --------\n",
      "-------- epoch = 2/150 --------\n",
      "batch: 1; loss = 0.5780501365661621\n",
      "-------- loss = 0.5780501365661621 --------\n",
      "-------- epoch = 3/150 --------\n",
      "batch: 1; loss = 0.5547901391983032\n",
      "-------- loss = 0.5547901391983032 --------\n",
      "-------- epoch = 4/150 --------\n",
      "batch: 1; loss = 0.5550546646118164\n",
      "-------- loss = 0.5550546646118164 --------\n",
      "-------- epoch = 5/150 --------\n",
      "batch: 1; loss = 0.5562398433685303\n",
      "-------- loss = 0.5562398433685303 --------\n",
      "-------- epoch = 6/150 --------\n",
      "batch: 1; loss = 0.5547587871551514\n",
      "-------- loss = 0.5547587871551514 --------\n",
      "-------- epoch = 7/150 --------\n",
      "batch: 1; loss = 0.553890585899353\n",
      "-------- loss = 0.553890585899353 --------\n",
      "-------- epoch = 8/150 --------\n",
      "batch: 1; loss = 0.5559033751487732\n",
      "-------- loss = 0.5559033751487732 --------\n",
      "-------- epoch = 9/150 --------\n",
      "batch: 1; loss = 0.5600392818450928\n",
      "-------- loss = 0.5600392818450928 --------\n",
      "-------- epoch = 10/150 --------\n",
      "batch: 1; loss = 0.5638548135757446\n",
      "-------- loss = 0.5638548135757446 --------\n",
      "-------- epoch = 11/150 --------\n",
      "batch: 1; loss = 0.5653755068778992\n",
      "-------- loss = 0.5653755068778992 --------\n",
      "-------- epoch = 12/150 --------\n",
      "batch: 1; loss = 0.5641312003135681\n",
      "-------- loss = 0.5641312003135681 --------\n",
      "-------- epoch = 13/150 --------\n",
      "batch: 1; loss = 0.5610389709472656\n",
      "-------- loss = 0.5610389709472656 --------\n",
      "-------- epoch = 14/150 --------\n",
      "batch: 1; loss = 0.5576473474502563\n",
      "-------- loss = 0.5576473474502563 --------\n",
      "-------- epoch = 15/150 --------\n",
      "batch: 1; loss = 0.5552189350128174\n",
      "-------- loss = 0.5552189350128174 --------\n",
      "-------- epoch = 16/150 --------\n",
      "batch: 1; loss = 0.5541166663169861\n",
      "-------- loss = 0.5541166663169861 --------\n",
      "-------- epoch = 17/150 --------\n",
      "batch: 1; loss = 0.5538707971572876\n",
      "-------- loss = 0.5538707971572876 --------\n",
      "-------- epoch = 18/150 --------\n",
      "batch: 1; loss = 0.553860604763031\n",
      "-------- loss = 0.553860604763031 --------\n",
      "-------- epoch = 19/150 --------\n",
      "batch: 1; loss = 0.5539480447769165\n",
      "-------- loss = 0.5539480447769165 --------\n",
      "-------- epoch = 20/150 --------\n",
      "batch: 1; loss = 0.554446280002594\n",
      "-------- loss = 0.554446280002594 --------\n",
      "-------- epoch = 21/150 --------\n",
      "batch: 1; loss = 0.5555607080459595\n",
      "-------- loss = 0.5555607080459595 --------\n",
      "-------- epoch = 22/150 --------\n",
      "batch: 1; loss = 0.5569637417793274\n",
      "-------- loss = 0.5569637417793274 --------\n",
      "-------- epoch = 23/150 --------\n",
      "batch: 1; loss = 0.5579460859298706\n",
      "-------- loss = 0.5579460859298706 --------\n",
      "-------- epoch = 24/150 --------\n",
      "batch: 1; loss = 0.5579708218574524\n",
      "-------- loss = 0.5579708218574524 --------\n",
      "-------- epoch = 25/150 --------\n",
      "batch: 1; loss = 0.5570838451385498\n",
      "-------- loss = 0.5570838451385498 --------\n",
      "-------- epoch = 26/150 --------\n",
      "batch: 1; loss = 0.555831789970398\n",
      "-------- loss = 0.555831789970398 --------\n",
      "-------- epoch = 27/150 --------\n",
      "batch: 1; loss = 0.5548093318939209\n",
      "-------- loss = 0.5548093318939209 --------\n",
      "-------- epoch = 28/150 --------\n",
      "batch: 1; loss = 0.554263174533844\n",
      "-------- loss = 0.554263174533844 --------\n",
      "-------- epoch = 29/150 --------\n",
      "batch: 1; loss = 0.5540938973426819\n",
      "-------- loss = 0.5540938973426819 --------\n",
      "-------- epoch = 30/150 --------\n",
      "batch: 1; loss = 0.5541680455207825\n",
      "-------- loss = 0.5541680455207825 --------\n",
      "-------- epoch = 31/150 --------\n",
      "batch: 1; loss = 0.5545136332511902\n",
      "-------- loss = 0.5545136332511902 --------\n",
      "-------- epoch = 32/150 --------\n",
      "batch: 1; loss = 0.5551729798316956\n",
      "-------- loss = 0.5551729798316956 --------\n",
      "-------- epoch = 33/150 --------\n",
      "batch: 1; loss = 0.5559514164924622\n",
      "-------- loss = 0.5559514164924622 --------\n",
      "-------- epoch = 34/150 --------\n",
      "batch: 1; loss = 0.5564433932304382\n",
      "-------- loss = 0.5564433932304382 --------\n",
      "-------- epoch = 35/150 --------\n",
      "batch: 1; loss = 0.5563589334487915\n",
      "-------- loss = 0.5563589334487915 --------\n",
      "-------- epoch = 36/150 --------\n",
      "batch: 1; loss = 0.5557853579521179\n",
      "-------- loss = 0.5557853579521179 --------\n",
      "-------- epoch = 37/150 --------\n",
      "batch: 1; loss = 0.5550878047943115\n",
      "-------- loss = 0.5550878047943115 --------\n",
      "-------- epoch = 38/150 --------\n",
      "batch: 1; loss = 0.5545842051506042\n",
      "-------- loss = 0.5545842051506042 --------\n",
      "-------- epoch = 39/150 --------\n",
      "batch: 1; loss = 0.5543654561042786\n",
      "-------- loss = 0.5543654561042786 --------\n",
      "-------- epoch = 40/150 --------\n",
      "batch: 1; loss = 0.5543947219848633\n",
      "-------- loss = 0.5543947219848633 --------\n",
      "-------- epoch = 41/150 --------\n",
      "batch: 1; loss = 0.5546491146087646\n",
      "-------- loss = 0.5546491146087646 --------\n",
      "-------- epoch = 42/150 --------\n",
      "batch: 1; loss = 0.555079996585846\n",
      "-------- loss = 0.555079996585846 --------\n",
      "-------- epoch = 43/150 --------\n",
      "batch: 1; loss = 0.5555038452148438\n",
      "-------- loss = 0.5555038452148438 --------\n",
      "-------- epoch = 44/150 --------\n",
      "batch: 1; loss = 0.5556707978248596\n",
      "-------- loss = 0.5556707978248596 --------\n",
      "-------- epoch = 45/150 --------\n",
      "batch: 1; loss = 0.5554818511009216\n",
      "-------- loss = 0.5554818511009216 --------\n",
      "-------- epoch = 46/150 --------\n",
      "batch: 1; loss = 0.5550807118415833\n",
      "-------- loss = 0.5550807118415833 --------\n",
      "-------- epoch = 47/150 --------\n",
      "batch: 1; loss = 0.5547044277191162\n",
      "-------- loss = 0.5547044277191162 --------\n",
      "-------- epoch = 48/150 --------\n",
      "batch: 1; loss = 0.5544957518577576\n",
      "-------- loss = 0.5544957518577576 --------\n",
      "-------- epoch = 49/150 --------\n",
      "batch: 1; loss = 0.5544787645339966\n",
      "-------- loss = 0.5544787645339966 --------\n",
      "-------- epoch = 50/150 --------\n",
      "batch: 1; loss = 0.5546324849128723\n",
      "-------- loss = 0.5546324849128723 --------\n",
      "-------- epoch = 51/150 --------\n",
      "batch: 1; loss = 0.5548994541168213\n",
      "-------- loss = 0.5548994541168213 --------\n",
      "-------- epoch = 52/150 --------\n",
      "batch: 1; loss = 0.5551497936248779\n",
      "-------- loss = 0.5551497936248779 --------\n",
      "-------- epoch = 53/150 --------\n",
      "batch: 1; loss = 0.5552348494529724\n",
      "-------- loss = 0.5552348494529724 --------\n",
      "-------- epoch = 54/150 --------\n",
      "batch: 1; loss = 0.5551106333732605\n",
      "-------- loss = 0.5551106333732605 --------\n",
      "-------- epoch = 55/150 --------\n",
      "batch: 1; loss = 0.5548734068870544\n",
      "-------- loss = 0.5548734068870544 --------\n",
      "-------- epoch = 56/150 --------\n",
      "batch: 1; loss = 0.554664134979248\n",
      "-------- loss = 0.554664134979248 --------\n",
      "-------- epoch = 57/150 --------\n",
      "batch: 1; loss = 0.554568350315094\n",
      "-------- loss = 0.554568350315094 --------\n",
      "-------- epoch = 58/150 --------\n",
      "batch: 1; loss = 0.5546048283576965\n",
      "-------- loss = 0.5546048283576965 --------\n",
      "-------- epoch = 59/150 --------\n",
      "batch: 1; loss = 0.5547482967376709\n",
      "-------- loss = 0.5547482967376709 --------\n",
      "-------- epoch = 60/150 --------\n",
      "batch: 1; loss = 0.5549302697181702\n",
      "-------- loss = 0.5549302697181702 --------\n",
      "-------- epoch = 61/150 --------\n",
      "batch: 1; loss = 0.5550519824028015\n",
      "-------- loss = 0.5550519824028015 --------\n",
      "-------- epoch = 62/150 --------\n",
      "batch: 1; loss = 0.555046558380127\n",
      "-------- loss = 0.555046558380127 --------\n",
      "-------- epoch = 63/150 --------\n",
      "batch: 1; loss = 0.5549319982528687\n",
      "-------- loss = 0.5549319982528687 --------\n",
      "-------- epoch = 64/150 --------\n",
      "batch: 1; loss = 0.5547892451286316\n",
      "-------- loss = 0.5547892451286316 --------\n",
      "-------- epoch = 65/150 --------\n",
      "batch: 1; loss = 0.5546972751617432\n",
      "-------- loss = 0.5546972751617432 --------\n",
      "-------- epoch = 66/150 --------\n",
      "batch: 1; loss = 0.5546932220458984\n",
      "-------- loss = 0.5546932220458984 --------\n",
      "-------- epoch = 67/150 --------\n",
      "batch: 1; loss = 0.5547717809677124\n",
      "-------- loss = 0.5547717809677124 --------\n",
      "-------- epoch = 68/150 --------\n",
      "batch: 1; loss = 0.5548917651176453\n",
      "-------- loss = 0.5548917651176453 --------\n",
      "-------- epoch = 69/150 --------\n",
      "batch: 1; loss = 0.5549888610839844\n",
      "-------- loss = 0.5549888610839844 --------\n",
      "-------- epoch = 70/150 --------\n",
      "batch: 1; loss = 0.5550112128257751\n",
      "-------- loss = 0.5550112128257751 --------\n",
      "-------- epoch = 71/150 --------\n",
      "batch: 1; loss = 0.554955005645752\n",
      "-------- loss = 0.554955005645752 --------\n",
      "-------- epoch = 72/150 --------\n",
      "batch: 1; loss = 0.5548642873764038\n",
      "-------- loss = 0.5548642873764038 --------\n",
      "-------- epoch = 73/150 --------\n",
      "batch: 1; loss = 0.5547947287559509\n",
      "-------- loss = 0.5547947287559509 --------\n",
      "-------- epoch = 74/150 --------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch: 1; loss = 0.5547809600830078\n",
      "-------- loss = 0.5547809600830078 --------\n",
      "-------- epoch = 75/150 --------\n",
      "batch: 1; loss = 0.5548253655433655\n",
      "-------- loss = 0.5548253655433655 --------\n",
      "-------- epoch = 76/150 --------\n",
      "batch: 1; loss = 0.5549013614654541\n",
      "-------- loss = 0.5549013614654541 --------\n",
      "-------- epoch = 77/150 --------\n",
      "batch: 1; loss = 0.5549660921096802\n",
      "-------- loss = 0.5549660921096802 --------\n",
      "-------- epoch = 78/150 --------\n",
      "batch: 1; loss = 0.5549840331077576\n",
      "-------- loss = 0.5549840331077576 --------\n",
      "-------- epoch = 79/150 --------\n",
      "batch: 1; loss = 0.5549497008323669\n",
      "-------- loss = 0.5549497008323669 --------\n",
      "-------- epoch = 80/150 --------\n",
      "batch: 1; loss = 0.5548900365829468\n",
      "-------- loss = 0.5548900365829468 --------\n",
      "-------- epoch = 81/150 --------\n",
      "batch: 1; loss = 0.5548422932624817\n",
      "-------- loss = 0.5548422932624817 --------\n",
      "-------- epoch = 82/150 --------\n",
      "batch: 1; loss = 0.5548315048217773\n",
      "-------- loss = 0.5548315048217773 --------\n",
      "-------- epoch = 83/150 --------\n",
      "batch: 1; loss = 0.5548597574234009\n",
      "-------- loss = 0.5548597574234009 --------\n",
      "-------- epoch = 84/150 --------\n",
      "batch: 1; loss = 0.5549081563949585\n",
      "-------- loss = 0.5549081563949585 --------\n",
      "-------- epoch = 85/150 --------\n",
      "batch: 1; loss = 0.5549469590187073\n",
      "-------- loss = 0.5549469590187073 --------\n",
      "-------- epoch = 86/150 --------\n",
      "batch: 1; loss = 0.5549540519714355\n",
      "-------- loss = 0.5549540519714355 --------\n",
      "-------- epoch = 87/150 --------\n",
      "batch: 1; loss = 0.5549280643463135\n",
      "-------- loss = 0.5549280643463135 --------\n",
      "-------- epoch = 88/150 --------\n",
      "batch: 1; loss = 0.5548880696296692\n",
      "-------- loss = 0.5548880696296692 --------\n",
      "-------- epoch = 89/150 --------\n",
      "batch: 1; loss = 0.554858922958374\n",
      "-------- loss = 0.554858922958374 --------\n",
      "-------- epoch = 90/150 --------\n",
      "batch: 1; loss = 0.5548560619354248\n",
      "-------- loss = 0.5548560619354248 --------\n",
      "-------- epoch = 91/150 --------\n",
      "batch: 1; loss = 0.5548779368400574\n",
      "-------- loss = 0.5548779368400574 --------\n",
      "-------- epoch = 92/150 --------\n",
      "batch: 1; loss = 0.5549091696739197\n",
      "-------- loss = 0.5549091696739197 --------\n",
      "-------- epoch = 93/150 --------\n",
      "batch: 1; loss = 0.5549296140670776\n",
      "-------- loss = 0.5549296140670776 --------\n",
      "-------- epoch = 94/150 --------\n",
      "batch: 1; loss = 0.554927408695221\n",
      "-------- loss = 0.554927408695221 --------\n",
      "-------- epoch = 95/150 --------\n",
      "batch: 1; loss = 0.5549059510231018\n",
      "-------- loss = 0.5549059510231018 --------\n",
      "-------- epoch = 96/150 --------\n",
      "batch: 1; loss = 0.5548800826072693\n",
      "-------- loss = 0.5548800826072693 --------\n",
      "-------- epoch = 97/150 --------\n",
      "batch: 1; loss = 0.5548659563064575\n",
      "-------- loss = 0.5548659563064575 --------\n",
      "-------- epoch = 98/150 --------\n",
      "batch: 1; loss = 0.5548704266548157\n",
      "-------- loss = 0.5548704266548157 --------\n",
      "-------- epoch = 99/150 --------\n",
      "batch: 1; loss = 0.5548886656761169\n",
      "-------- loss = 0.5548886656761169 --------\n",
      "-------- epoch = 100/150 --------\n",
      "batch: 1; loss = 0.5549076199531555\n",
      "-------- loss = 0.5549076199531555 --------\n",
      "-------- epoch = 101/150 --------\n",
      "batch: 1; loss = 0.5549151301383972\n",
      "-------- loss = 0.5549151301383972 --------\n",
      "-------- epoch = 102/150 --------\n",
      "batch: 1; loss = 0.5549073815345764\n",
      "-------- loss = 0.5549073815345764 --------\n",
      "-------- epoch = 103/150 --------\n",
      "batch: 1; loss = 0.5548907518386841\n",
      "-------- loss = 0.5548907518386841 --------\n",
      "-------- epoch = 104/150 --------\n",
      "batch: 1; loss = 0.5548768639564514\n",
      "-------- loss = 0.5548768639564514 --------\n",
      "-------- epoch = 105/150 --------\n",
      "batch: 1; loss = 0.554874062538147\n",
      "-------- loss = 0.554874062538147 --------\n",
      "-------- epoch = 106/150 --------\n",
      "batch: 1; loss = 0.5548831224441528\n",
      "-------- loss = 0.5548831224441528 --------\n",
      "-------- epoch = 107/150 --------\n",
      "batch: 1; loss = 0.5548969507217407\n",
      "-------- loss = 0.5548969507217407 --------\n",
      "-------- epoch = 108/150 --------\n",
      "batch: 1; loss = 0.55490642786026\n",
      "-------- loss = 0.55490642786026 --------\n",
      "-------- epoch = 109/150 --------\n",
      "batch: 1; loss = 0.5549057722091675\n",
      "-------- loss = 0.5549057722091675 --------\n",
      "-------- epoch = 110/150 --------\n",
      "batch: 1; loss = 0.5548967719078064\n",
      "-------- loss = 0.5548967719078064 --------\n",
      "-------- epoch = 111/150 --------\n",
      "batch: 1; loss = 0.5548866987228394\n",
      "-------- loss = 0.5548866987228394 --------\n",
      "-------- epoch = 112/150 --------\n",
      "batch: 1; loss = 0.5548824667930603\n",
      "-------- loss = 0.5548824667930603 --------\n",
      "-------- epoch = 113/150 --------\n",
      "batch: 1; loss = 0.5548866987228394\n",
      "-------- loss = 0.5548866987228394 --------\n",
      "-------- epoch = 114/150 --------\n",
      "batch: 1; loss = 0.5548961758613586\n",
      "-------- loss = 0.5548961758613586 --------\n",
      "-------- epoch = 115/150 --------\n",
      "batch: 1; loss = 0.5549044013023376\n",
      "-------- loss = 0.5549044013023376 --------\n",
      "-------- epoch = 116/150 --------\n",
      "batch: 1; loss = 0.5549064874649048\n",
      "-------- loss = 0.5549064874649048 --------\n",
      "-------- epoch = 117/150 --------\n",
      "batch: 1; loss = 0.5549023151397705\n",
      "-------- loss = 0.5549023151397705 --------\n",
      "-------- epoch = 118/150 --------\n",
      "batch: 1; loss = 0.5548956990242004\n",
      "-------- loss = 0.5548956990242004 --------\n",
      "-------- epoch = 119/150 --------\n",
      "batch: 1; loss = 0.5548921227455139\n",
      "-------- loss = 0.5548921227455139 --------\n",
      "-------- epoch = 120/150 --------\n",
      "batch: 1; loss = 0.5548940896987915\n",
      "-------- loss = 0.5548940896987915 --------\n",
      "-------- epoch = 121/150 --------\n",
      "batch: 1; loss = 0.5549003481864929\n",
      "-------- loss = 0.5549003481864929 --------\n",
      "-------- epoch = 122/150 --------\n",
      "batch: 1; loss = 0.5549066066741943\n",
      "-------- loss = 0.5549066066741943 --------\n",
      "-------- epoch = 123/150 --------\n",
      "batch: 1; loss = 0.5549092888832092\n",
      "-------- loss = 0.5549092888832092 --------\n",
      "-------- epoch = 124/150 --------\n",
      "batch: 1; loss = 0.5549073219299316\n",
      "-------- loss = 0.5549073219299316 --------\n",
      "-------- epoch = 125/150 --------\n",
      "batch: 1; loss = 0.5549032688140869\n",
      "-------- loss = 0.5549032688140869 --------\n",
      "-------- epoch = 126/150 --------\n",
      "batch: 1; loss = 0.5549007058143616\n",
      "-------- loss = 0.5549007058143616 --------\n",
      "-------- epoch = 127/150 --------\n",
      "batch: 1; loss = 0.5549017190933228\n",
      "-------- loss = 0.5549017190933228 --------\n",
      "-------- epoch = 128/150 --------\n",
      "batch: 1; loss = 0.5549057722091675\n",
      "-------- loss = 0.5549057722091675 --------\n",
      "-------- epoch = 129/150 --------\n",
      "batch: 1; loss = 0.5549102425575256\n",
      "-------- loss = 0.5549102425575256 --------\n",
      "-------- epoch = 130/150 --------\n",
      "batch: 1; loss = 0.5549123287200928\n",
      "-------- loss = 0.5549123287200928 --------\n",
      "-------- epoch = 131/150 --------\n",
      "batch: 1; loss = 0.5549113750457764\n",
      "-------- loss = 0.5549113750457764 --------\n",
      "-------- epoch = 132/150 --------\n",
      "batch: 1; loss = 0.5549087524414062\n",
      "-------- loss = 0.5549087524414062 --------\n",
      "-------- epoch = 133/150 --------\n",
      "batch: 1; loss = 0.5549070239067078\n",
      "-------- loss = 0.5549070239067078 --------\n",
      "-------- epoch = 134/150 --------\n",
      "batch: 1; loss = 0.5549077391624451\n",
      "-------- loss = 0.5549077391624451 --------\n",
      "-------- epoch = 135/150 --------\n",
      "batch: 1; loss = 0.5549104809761047\n",
      "-------- loss = 0.5549104809761047 --------\n",
      "-------- epoch = 136/150 --------\n",
      "batch: 1; loss = 0.5549135208129883\n",
      "-------- loss = 0.5549135208129883 --------\n",
      "-------- epoch = 137/150 --------\n",
      "batch: 1; loss = 0.5549150109291077\n",
      "-------- loss = 0.5549150109291077 --------\n",
      "-------- epoch = 138/150 --------\n",
      "batch: 1; loss = 0.5549144148826599\n",
      "-------- loss = 0.5549144148826599 --------\n",
      "-------- epoch = 139/150 --------\n",
      "batch: 1; loss = 0.554912805557251\n",
      "-------- loss = 0.554912805557251 --------\n",
      "-------- epoch = 140/150 --------\n",
      "batch: 1; loss = 0.554911732673645\n",
      "-------- loss = 0.554911732673645 --------\n",
      "-------- epoch = 141/150 --------\n",
      "batch: 1; loss = 0.5549124479293823\n",
      "-------- loss = 0.5549124479293823 --------\n",
      "-------- epoch = 142/150 --------\n",
      "batch: 1; loss = 0.5549144148826599\n",
      "-------- loss = 0.5549144148826599 --------\n",
      "-------- epoch = 143/150 --------\n",
      "batch: 1; loss = 0.554916501045227\n",
      "-------- loss = 0.554916501045227 --------\n",
      "-------- epoch = 144/150 --------\n",
      "batch: 1; loss = 0.5549174547195435\n",
      "-------- loss = 0.5549174547195435 --------\n",
      "-------- epoch = 145/150 --------\n",
      "batch: 1; loss = 0.55491703748703\n",
      "-------- loss = 0.55491703748703 --------\n",
      "-------- epoch = 146/150 --------\n",
      "batch: 1; loss = 0.5549160242080688\n",
      "-------- loss = 0.5549160242080688 --------\n",
      "-------- epoch = 147/150 --------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch: 1; loss = 0.5549156665802002\n",
      "-------- loss = 0.5549156665802002 --------\n",
      "-------- epoch = 148/150 --------\n",
      "batch: 1; loss = 0.5549163222312927\n",
      "-------- loss = 0.5549163222312927 --------\n",
      "-------- epoch = 149/150 --------\n",
      "batch: 1; loss = 0.5549178719520569\n",
      "-------- loss = 0.5549178719520569 --------\n",
      "-------- epoch = 150/150 --------\n",
      "batch: 1; loss = 0.5549193620681763\n",
      "-------- loss = 0.5549193620681763 --------\n"
     ]
    }
   ],
   "source": [
    "model_w2v.fit(data_w2v, batch_size, epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "9f481543",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(model, data, metric=True, i=0,):\n",
    "    if metric:\n",
    "        dl = torch.utils.data.DataLoader(data, batch_size=len(data))\n",
    "        for X, Y in dl:\n",
    "            Y_pred = []\n",
    "            arr = torch.reshape(model(X).cpu(), (len(data), )).detach().numpy()\n",
    "            for i in arr:\n",
    "                if i >= 0.3:\n",
    "                    Y_pred.append(1)\n",
    "                elif i < 0.3:\n",
    "                    Y_pred.append(0) \n",
    "            rec = metrics.recall_score(Y.cpu().detach().numpy(), np.array(Y_pred))\n",
    "            prec = metrics.precision_score(Y.cpu().detach().numpy(), np.array(Y_pred))\n",
    "        print(f'recall = {rec}; precision = {prec}')\n",
    "    else:\n",
    "        x, y = data[i]\n",
    "        x = torch.reshape(x, (1, 44))\n",
    "        print(f'predicted = {model(x)}: true = {y}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5d9edb8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model, 'model.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "4dcc4f60",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'DataPreprocessing' object has no attribute 'to'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn [68], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m device \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mdevice(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcuda:0\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mis_available() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m      2\u001b[0m model\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m----> 3\u001b[0m \u001b[43mdata\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m(device)\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'DataPreprocessing' object has no attribute 'to'"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "200a2e58",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "recall = 1.0; precision = 0.24242424242424243\n"
     ]
    }
   ],
   "source": [
    "predict(model, data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "3d16757d",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_test = DataPreprocessing_self_emb(df_data, subset='test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "f1f52ba8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "recall = 1.0; precision = 0.25\n"
     ]
    }
   ],
   "source": [
    "predict(model, data_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "57395dfe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "recall = 0.0; precision = 0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Эдуард\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1327: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "predict(model_w2v, data_w2v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "6c06bea4",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_test_w2v = DataPreprocessing(df_data, subset='test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "26c9b9ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "recall = 0.0; precision = 0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Эдуард\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1327: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "predict(model_w2v, data_test_w2v)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca480067",
   "metadata": {},
   "source": [
    "## Проба tf-idf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "0c3d9029",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "6e21ef5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataPreprocessing_tf_idf(torch.utils.data.Dataset):\n",
    "    def __init__(self, df, subset='train'):\n",
    "        self.subset = subset\n",
    "        self.length_of_sentense = 32\n",
    "        self.vectorizer = TfidfVectorizer()\n",
    "        self.vectorizer.fit(df['Описание анкеты'].values)\n",
    "        #self.x_train, self.x_test, self.y_train, self.y_test = model_selection.train_test_split(self.df['Описание анкеты'], self.df[' Рекомендовать'], test_size=0.33, random_state=42)\n",
    "        df_t = df[df[' Рекомендовать'] == 1].iloc[0:16]\n",
    "        df_f = df[df[' Рекомендовать'] == 0].iloc[0:48]\n",
    "        df_test = pd.concat([df_t, df_f], ignore_index=True)\n",
    "        self.x_test = df_test['Описание анкеты']\n",
    "        self.y_test = df_test[' Рекомендовать']\n",
    "        train = df.drop(index=df_t.index.append(df_f.index))\n",
    "        self.x_train = train['Описание анкеты']\n",
    "        self.y_train = train[' Рекомендовать']\n",
    "        \n",
    "    def vectorize(self, disc):\n",
    "        tokens = disc.split(' ')\n",
    "        tf_idf = self.vectorizer.transform(tokens).toarray()\n",
    "        if tf_idf.shape[0] < self.length_of_sentense:\n",
    "            addition = np.zeros((self.length_of_sentense-tf_idf.shape[0], tf_idf.shape[1]), dtype=np.float32)\n",
    "            tf_idf = np.vstack((tf_idf, addition))\n",
    "        tf_idf = np.sum(tf_idf, axis=0, dtype=np.float32)\n",
    "        return torch.from_numpy(tf_idf).cuda()\n",
    "                \n",
    "    def __len__(self):\n",
    "        if self.subset == 'train':\n",
    "            return len(self.x_train)\n",
    "        else:\n",
    "            return len(self.x_test)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        if self.subset == 'train':\n",
    "            disc = self.x_train.values[index]\n",
    "            recomend = self.y_train.values[index].astype(float)\n",
    "        else:\n",
    "            disc = self.x_test.values[index]\n",
    "            recomend = self.y_test.values[index].astype(float)\n",
    "        if type(disc) is str:\n",
    "            return self.vectorize(disc), torch.tensor(recomend, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "b098ae9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_tf_idf = DataPreprocessing_tf_idf(df_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "ee41c6f5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([626])"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_tf_idf[0][0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "4a53fc7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model_tf_idf(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__() \n",
    "        self.lin1 = torch.nn.Linear(626, 500)\n",
    "        self.lin2 = torch.nn.Linear(500, 100)\n",
    "        self.lin3 = torch.nn.Linear(100, 1)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        y = torch.nn.functional.tanh(self.lin1(x))\n",
    "        y = torch.nn.functional.relu(self.lin2(y))\n",
    "        y = torch.sigmoid(self.lin3(y))\n",
    "        return y\n",
    "    \n",
    "    def fit(self, data, batch_size, epochs):\n",
    "        dl = torch.utils.data.DataLoader(data, batch_size=batch_size)\n",
    "        loss_fn = torch.nn.BCELoss()\n",
    "        optimizer = torch.optim.Adam(self.parameters(), weight_decay=0.1)\n",
    "        self.train()\n",
    "        for epoch in range(epochs):\n",
    "            Loss = 0\n",
    "            print(f'-------- epoch = {epoch+1}/{epochs} --------')\n",
    "            for batch, (X, Y) in enumerate(dl):\n",
    "                optimizer.zero_grad()\n",
    "                pred = self(X).to(device)###\n",
    "                target = torch.reshape(Y.to(device), (-1, 1)).float()\n",
    "                loss = loss_fn(pred, target)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                if batch % 1 == 0:\n",
    "                    loss, current = loss.item(), (batch + 1)*len(X)\n",
    "                    Loss += loss\n",
    "                    print(f'batch: {batch+1}; loss = {loss}')\n",
    "            print(f'-------- loss = {Loss/(int(len(data)/batch_size))} --------')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "30a5513c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------- epoch = 1/150 --------\n",
      "batch: 1; loss = 0.7071990966796875\n",
      "-------- loss = 0.7071990966796875 --------\n",
      "-------- epoch = 2/150 --------\n",
      "batch: 1; loss = 0.7034494876861572\n",
      "-------- loss = 0.7034494876861572 --------\n",
      "-------- epoch = 3/150 --------\n",
      "batch: 1; loss = 0.7000429630279541\n",
      "-------- loss = 0.7000429630279541 --------\n",
      "-------- epoch = 4/150 --------\n",
      "batch: 1; loss = 0.6969481110572815\n",
      "-------- loss = 0.6969481110572815 --------\n",
      "-------- epoch = 5/150 --------\n",
      "batch: 1; loss = 0.6941184997558594\n",
      "-------- loss = 0.6941184997558594 --------\n",
      "-------- epoch = 6/150 --------\n",
      "batch: 1; loss = 0.6915236115455627\n",
      "-------- loss = 0.6915236115455627 --------\n",
      "-------- epoch = 7/150 --------\n",
      "batch: 1; loss = 0.6891514658927917\n",
      "-------- loss = 0.6891514658927917 --------\n",
      "-------- epoch = 8/150 --------\n",
      "batch: 1; loss = 0.6869798302650452\n",
      "-------- loss = 0.6869798302650452 --------\n",
      "-------- epoch = 9/150 --------\n",
      "batch: 1; loss = 0.6849488615989685\n",
      "-------- loss = 0.6849488615989685 --------\n",
      "-------- epoch = 10/150 --------\n",
      "batch: 1; loss = 0.683043897151947\n",
      "-------- loss = 0.683043897151947 --------\n",
      "-------- epoch = 11/150 --------\n",
      "batch: 1; loss = 0.681244969367981\n",
      "-------- loss = 0.681244969367981 --------\n",
      "-------- epoch = 12/150 --------\n",
      "batch: 1; loss = 0.6795400381088257\n",
      "-------- loss = 0.6795400381088257 --------\n",
      "-------- epoch = 13/150 --------\n",
      "batch: 1; loss = 0.6779043078422546\n",
      "-------- loss = 0.6779043078422546 --------\n",
      "-------- epoch = 14/150 --------\n",
      "batch: 1; loss = 0.6763252019882202\n",
      "-------- loss = 0.6763252019882202 --------\n",
      "-------- epoch = 15/150 --------\n",
      "batch: 1; loss = 0.6747905611991882\n",
      "-------- loss = 0.6747905611991882 --------\n",
      "-------- epoch = 16/150 --------\n",
      "batch: 1; loss = 0.673291802406311\n",
      "-------- loss = 0.673291802406311 --------\n",
      "-------- epoch = 17/150 --------\n",
      "batch: 1; loss = 0.6718268394470215\n",
      "-------- loss = 0.6718268394470215 --------\n",
      "-------- epoch = 18/150 --------\n",
      "batch: 1; loss = 0.6703751683235168\n",
      "-------- loss = 0.6703751683235168 --------\n",
      "-------- epoch = 19/150 --------\n",
      "batch: 1; loss = 0.6689316630363464\n",
      "-------- loss = 0.6689316630363464 --------\n",
      "-------- epoch = 20/150 --------\n",
      "batch: 1; loss = 0.6674874424934387\n",
      "-------- loss = 0.6674874424934387 --------\n",
      "-------- epoch = 21/150 --------\n",
      "batch: 1; loss = 0.6660395860671997\n",
      "-------- loss = 0.6660395860671997 --------\n",
      "-------- epoch = 22/150 --------\n",
      "batch: 1; loss = 0.6645857095718384\n",
      "-------- loss = 0.6645857095718384 --------\n",
      "-------- epoch = 23/150 --------\n",
      "batch: 1; loss = 0.6631218791007996\n",
      "-------- loss = 0.6631218791007996 --------\n",
      "-------- epoch = 24/150 --------\n",
      "batch: 1; loss = 0.6616423726081848\n",
      "-------- loss = 0.6616423726081848 --------\n",
      "-------- epoch = 25/150 --------\n",
      "batch: 1; loss = 0.6601433157920837\n",
      "-------- loss = 0.6601433157920837 --------\n",
      "-------- epoch = 26/150 --------\n",
      "batch: 1; loss = 0.6586223244667053\n",
      "-------- loss = 0.6586223244667053 --------\n",
      "-------- epoch = 27/150 --------\n",
      "batch: 1; loss = 0.6570757031440735\n",
      "-------- loss = 0.6570757031440735 --------\n",
      "-------- epoch = 28/150 --------\n",
      "batch: 1; loss = 0.6554950475692749\n",
      "-------- loss = 0.6554950475692749 --------\n",
      "-------- epoch = 29/150 --------\n",
      "batch: 1; loss = 0.6538702249526978\n",
      "-------- loss = 0.6538702249526978 --------\n",
      "-------- epoch = 30/150 --------\n",
      "batch: 1; loss = 0.6521965861320496\n",
      "-------- loss = 0.6521965861320496 --------\n",
      "-------- epoch = 31/150 --------\n",
      "batch: 1; loss = 0.6504722237586975\n",
      "-------- loss = 0.6504722237586975 --------\n",
      "-------- epoch = 32/150 --------\n",
      "batch: 1; loss = 0.6486989855766296\n",
      "-------- loss = 0.6486989855766296 --------\n",
      "-------- epoch = 33/150 --------\n",
      "batch: 1; loss = 0.6468753218650818\n",
      "-------- loss = 0.6468753218650818 --------\n",
      "-------- epoch = 34/150 --------\n",
      "batch: 1; loss = 0.6449998021125793\n",
      "-------- loss = 0.6449998021125793 --------\n",
      "-------- epoch = 35/150 --------\n",
      "batch: 1; loss = 0.6430706977844238\n",
      "-------- loss = 0.6430706977844238 --------\n",
      "-------- epoch = 36/150 --------\n",
      "batch: 1; loss = 0.6410854458808899\n",
      "-------- loss = 0.6410854458808899 --------\n",
      "-------- epoch = 37/150 --------\n",
      "batch: 1; loss = 0.6390416026115417\n",
      "-------- loss = 0.6390416026115417 --------\n",
      "-------- epoch = 38/150 --------\n",
      "batch: 1; loss = 0.6369370222091675\n",
      "-------- loss = 0.6369370222091675 --------\n",
      "-------- epoch = 39/150 --------\n",
      "batch: 1; loss = 0.6347692012786865\n",
      "-------- loss = 0.6347692012786865 --------\n",
      "-------- epoch = 40/150 --------\n",
      "batch: 1; loss = 0.6325353384017944\n",
      "-------- loss = 0.6325353384017944 --------\n",
      "-------- epoch = 41/150 --------\n",
      "batch: 1; loss = 0.6302335262298584\n",
      "-------- loss = 0.6302335262298584 --------\n",
      "-------- epoch = 42/150 --------\n",
      "batch: 1; loss = 0.6278623938560486\n",
      "-------- loss = 0.6278623938560486 --------\n",
      "-------- epoch = 43/150 --------\n",
      "batch: 1; loss = 0.6254212260246277\n",
      "-------- loss = 0.6254212260246277 --------\n",
      "-------- epoch = 44/150 --------\n",
      "batch: 1; loss = 0.6229109764099121\n",
      "-------- loss = 0.6229109764099121 --------\n",
      "-------- epoch = 45/150 --------\n",
      "batch: 1; loss = 0.6203312873840332\n",
      "-------- loss = 0.6203312873840332 --------\n",
      "-------- epoch = 46/150 --------\n",
      "batch: 1; loss = 0.61768639087677\n",
      "-------- loss = 0.61768639087677 --------\n",
      "-------- epoch = 47/150 --------\n",
      "batch: 1; loss = 0.6149797439575195\n",
      "-------- loss = 0.6149797439575195 --------\n",
      "-------- epoch = 48/150 --------\n",
      "batch: 1; loss = 0.6122155785560608\n",
      "-------- loss = 0.6122155785560608 --------\n",
      "-------- epoch = 49/150 --------\n",
      "batch: 1; loss = 0.6093989610671997\n",
      "-------- loss = 0.6093989610671997 --------\n",
      "-------- epoch = 50/150 --------\n",
      "batch: 1; loss = 0.6065371632575989\n",
      "-------- loss = 0.6065371632575989 --------\n",
      "-------- epoch = 51/150 --------\n",
      "batch: 1; loss = 0.6036410331726074\n",
      "-------- loss = 0.6036410331726074 --------\n",
      "-------- epoch = 52/150 --------\n",
      "batch: 1; loss = 0.6007159352302551\n",
      "-------- loss = 0.6007159352302551 --------\n",
      "-------- epoch = 53/150 --------\n",
      "batch: 1; loss = 0.5977767705917358\n",
      "-------- loss = 0.5977767705917358 --------\n",
      "-------- epoch = 54/150 --------\n",
      "batch: 1; loss = 0.5948340892791748\n",
      "-------- loss = 0.5948340892791748 --------\n",
      "-------- epoch = 55/150 --------\n",
      "batch: 1; loss = 0.5919011831283569\n",
      "-------- loss = 0.5919011831283569 --------\n",
      "-------- epoch = 56/150 --------\n",
      "batch: 1; loss = 0.588992178440094\n",
      "-------- loss = 0.588992178440094 --------\n",
      "-------- epoch = 57/150 --------\n",
      "batch: 1; loss = 0.5861221551895142\n",
      "-------- loss = 0.5861221551895142 --------\n",
      "-------- epoch = 58/150 --------\n",
      "batch: 1; loss = 0.5833053588867188\n",
      "-------- loss = 0.5833053588867188 --------\n",
      "-------- epoch = 59/150 --------\n",
      "batch: 1; loss = 0.5805565714836121\n",
      "-------- loss = 0.5805565714836121 --------\n",
      "-------- epoch = 60/150 --------\n",
      "batch: 1; loss = 0.5778889656066895\n",
      "-------- loss = 0.5778889656066895 --------\n",
      "-------- epoch = 61/150 --------\n",
      "batch: 1; loss = 0.5753145217895508\n",
      "-------- loss = 0.5753145217895508 --------\n",
      "-------- epoch = 62/150 --------\n",
      "batch: 1; loss = 0.5728459358215332\n",
      "-------- loss = 0.5728459358215332 --------\n",
      "-------- epoch = 63/150 --------\n",
      "batch: 1; loss = 0.5704922676086426\n",
      "-------- loss = 0.5704922676086426 --------\n",
      "-------- epoch = 64/150 --------\n",
      "batch: 1; loss = 0.5682629346847534\n",
      "-------- loss = 0.5682629346847534 --------\n",
      "-------- epoch = 65/150 --------\n",
      "batch: 1; loss = 0.5661594867706299\n",
      "-------- loss = 0.5661594867706299 --------\n",
      "-------- epoch = 66/150 --------\n",
      "batch: 1; loss = 0.5641874074935913\n",
      "-------- loss = 0.5641874074935913 --------\n",
      "-------- epoch = 67/150 --------\n",
      "batch: 1; loss = 0.5623467564582825\n",
      "-------- loss = 0.5623467564582825 --------\n",
      "-------- epoch = 68/150 --------\n",
      "batch: 1; loss = 0.5606355667114258\n",
      "-------- loss = 0.5606355667114258 --------\n",
      "-------- epoch = 69/150 --------\n",
      "batch: 1; loss = 0.5590495467185974\n",
      "-------- loss = 0.5590495467185974 --------\n",
      "-------- epoch = 70/150 --------\n",
      "batch: 1; loss = 0.5575838685035706\n",
      "-------- loss = 0.5575838685035706 --------\n",
      "-------- epoch = 71/150 --------\n",
      "batch: 1; loss = 0.5562320351600647\n",
      "-------- loss = 0.5562320351600647 --------\n",
      "-------- epoch = 72/150 --------\n",
      "batch: 1; loss = 0.5549881458282471\n",
      "-------- loss = 0.5549881458282471 --------\n",
      "-------- epoch = 73/150 --------\n",
      "batch: 1; loss = 0.5538403987884521\n",
      "-------- loss = 0.5538403987884521 --------\n",
      "-------- epoch = 74/150 --------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch: 1; loss = 0.5527858138084412\n",
      "-------- loss = 0.5527858138084412 --------\n",
      "-------- epoch = 75/150 --------\n",
      "batch: 1; loss = 0.5518147945404053\n",
      "-------- loss = 0.5518147945404053 --------\n",
      "-------- epoch = 76/150 --------\n",
      "batch: 1; loss = 0.5509194135665894\n",
      "-------- loss = 0.5509194135665894 --------\n",
      "-------- epoch = 77/150 --------\n",
      "batch: 1; loss = 0.5500921010971069\n",
      "-------- loss = 0.5500921010971069 --------\n",
      "-------- epoch = 78/150 --------\n",
      "batch: 1; loss = 0.5493262410163879\n",
      "-------- loss = 0.5493262410163879 --------\n",
      "-------- epoch = 79/150 --------\n",
      "batch: 1; loss = 0.5486149787902832\n",
      "-------- loss = 0.5486149787902832 --------\n",
      "-------- epoch = 80/150 --------\n",
      "batch: 1; loss = 0.5479522943496704\n",
      "-------- loss = 0.5479522943496704 --------\n",
      "-------- epoch = 81/150 --------\n",
      "batch: 1; loss = 0.5473331212997437\n",
      "-------- loss = 0.5473331212997437 --------\n",
      "-------- epoch = 82/150 --------\n",
      "batch: 1; loss = 0.5467525720596313\n",
      "-------- loss = 0.5467525720596313 --------\n",
      "-------- epoch = 83/150 --------\n",
      "batch: 1; loss = 0.5462064146995544\n",
      "-------- loss = 0.5462064146995544 --------\n",
      "-------- epoch = 84/150 --------\n",
      "batch: 1; loss = 0.5456905961036682\n",
      "-------- loss = 0.5456905961036682 --------\n",
      "-------- epoch = 85/150 --------\n",
      "batch: 1; loss = 0.54520183801651\n",
      "-------- loss = 0.54520183801651 --------\n",
      "-------- epoch = 86/150 --------\n",
      "batch: 1; loss = 0.5447373986244202\n",
      "-------- loss = 0.5447373986244202 --------\n",
      "-------- epoch = 87/150 --------\n",
      "batch: 1; loss = 0.5442954897880554\n",
      "-------- loss = 0.5442954897880554 --------\n",
      "-------- epoch = 88/150 --------\n",
      "batch: 1; loss = 0.5438746809959412\n",
      "-------- loss = 0.5438746809959412 --------\n",
      "-------- epoch = 89/150 --------\n",
      "batch: 1; loss = 0.5434737801551819\n",
      "-------- loss = 0.5434737801551819 --------\n",
      "-------- epoch = 90/150 --------\n",
      "batch: 1; loss = 0.5430922508239746\n",
      "-------- loss = 0.5430922508239746 --------\n",
      "-------- epoch = 91/150 --------\n",
      "batch: 1; loss = 0.5427290201187134\n",
      "-------- loss = 0.5427290201187134 --------\n",
      "-------- epoch = 92/150 --------\n",
      "batch: 1; loss = 0.5423840880393982\n",
      "-------- loss = 0.5423840880393982 --------\n",
      "-------- epoch = 93/150 --------\n",
      "batch: 1; loss = 0.5420572757720947\n",
      "-------- loss = 0.5420572757720947 --------\n",
      "-------- epoch = 94/150 --------\n",
      "batch: 1; loss = 0.5417487621307373\n",
      "-------- loss = 0.5417487621307373 --------\n",
      "-------- epoch = 95/150 --------\n",
      "batch: 1; loss = 0.541458010673523\n",
      "-------- loss = 0.541458010673523 --------\n",
      "-------- epoch = 96/150 --------\n",
      "batch: 1; loss = 0.541185200214386\n",
      "-------- loss = 0.541185200214386 --------\n",
      "-------- epoch = 97/150 --------\n",
      "batch: 1; loss = 0.5409299731254578\n",
      "-------- loss = 0.5409299731254578 --------\n",
      "-------- epoch = 98/150 --------\n",
      "batch: 1; loss = 0.5406919717788696\n",
      "-------- loss = 0.5406919717788696 --------\n",
      "-------- epoch = 99/150 --------\n",
      "batch: 1; loss = 0.5404708385467529\n",
      "-------- loss = 0.5404708385467529 --------\n",
      "-------- epoch = 100/150 --------\n",
      "batch: 1; loss = 0.540266215801239\n",
      "-------- loss = 0.540266215801239 --------\n",
      "-------- epoch = 101/150 --------\n",
      "batch: 1; loss = 0.540077805519104\n",
      "-------- loss = 0.540077805519104 --------\n",
      "-------- epoch = 102/150 --------\n",
      "batch: 1; loss = 0.5399050116539001\n",
      "-------- loss = 0.5399050116539001 --------\n",
      "-------- epoch = 103/150 --------\n",
      "batch: 1; loss = 0.5397469401359558\n",
      "-------- loss = 0.5397469401359558 --------\n",
      "-------- epoch = 104/150 --------\n",
      "batch: 1; loss = 0.5396028161048889\n",
      "-------- loss = 0.5396028161048889 --------\n",
      "-------- epoch = 105/150 --------\n",
      "batch: 1; loss = 0.5394716858863831\n",
      "-------- loss = 0.5394716858863831 --------\n",
      "-------- epoch = 106/150 --------\n",
      "batch: 1; loss = 0.5393527746200562\n",
      "-------- loss = 0.5393527746200562 --------\n",
      "-------- epoch = 107/150 --------\n",
      "batch: 1; loss = 0.5392452478408813\n",
      "-------- loss = 0.5392452478408813 --------\n",
      "-------- epoch = 108/150 --------\n",
      "batch: 1; loss = 0.5391483902931213\n",
      "-------- loss = 0.5391483902931213 --------\n",
      "-------- epoch = 109/150 --------\n",
      "batch: 1; loss = 0.5390612483024597\n",
      "-------- loss = 0.5390612483024597 --------\n",
      "-------- epoch = 110/150 --------\n",
      "batch: 1; loss = 0.5389832258224487\n",
      "-------- loss = 0.5389832258224487 --------\n",
      "-------- epoch = 111/150 --------\n",
      "batch: 1; loss = 0.5389129519462585\n",
      "-------- loss = 0.5389129519462585 --------\n",
      "-------- epoch = 112/150 --------\n",
      "batch: 1; loss = 0.5388502478599548\n",
      "-------- loss = 0.5388502478599548 --------\n",
      "-------- epoch = 113/150 --------\n",
      "batch: 1; loss = 0.5387945771217346\n",
      "-------- loss = 0.5387945771217346 --------\n",
      "-------- epoch = 114/150 --------\n",
      "batch: 1; loss = 0.5387455224990845\n",
      "-------- loss = 0.5387455224990845 --------\n",
      "-------- epoch = 115/150 --------\n",
      "batch: 1; loss = 0.5387026071548462\n",
      "-------- loss = 0.5387026071548462 --------\n",
      "-------- epoch = 116/150 --------\n",
      "batch: 1; loss = 0.5386653542518616\n",
      "-------- loss = 0.5386653542518616 --------\n",
      "-------- epoch = 117/150 --------\n",
      "batch: 1; loss = 0.5386335253715515\n",
      "-------- loss = 0.5386335253715515 --------\n",
      "-------- epoch = 118/150 --------\n",
      "batch: 1; loss = 0.5386067032814026\n",
      "-------- loss = 0.5386067032814026 --------\n",
      "-------- epoch = 119/150 --------\n",
      "batch: 1; loss = 0.5385848879814148\n",
      "-------- loss = 0.5385848879814148 --------\n",
      "-------- epoch = 120/150 --------\n",
      "batch: 1; loss = 0.5385673642158508\n",
      "-------- loss = 0.5385673642158508 --------\n",
      "-------- epoch = 121/150 --------\n",
      "batch: 1; loss = 0.5385542511940002\n",
      "-------- loss = 0.5385542511940002 --------\n",
      "-------- epoch = 122/150 --------\n",
      "batch: 1; loss = 0.5385452508926392\n",
      "-------- loss = 0.5385452508926392 --------\n",
      "-------- epoch = 123/150 --------\n",
      "batch: 1; loss = 0.5385401844978333\n",
      "-------- loss = 0.5385401844978333 --------\n",
      "-------- epoch = 124/150 --------\n",
      "batch: 1; loss = 0.5385389924049377\n",
      "-------- loss = 0.5385389924049377 --------\n",
      "-------- epoch = 125/150 --------\n",
      "batch: 1; loss = 0.5385413765907288\n",
      "-------- loss = 0.5385413765907288 --------\n",
      "-------- epoch = 126/150 --------\n",
      "batch: 1; loss = 0.5385472178459167\n",
      "-------- loss = 0.5385472178459167 --------\n",
      "-------- epoch = 127/150 --------\n",
      "batch: 1; loss = 0.5385564565658569\n",
      "-------- loss = 0.5385564565658569 --------\n",
      "-------- epoch = 128/150 --------\n",
      "batch: 1; loss = 0.538568913936615\n",
      "-------- loss = 0.538568913936615 --------\n",
      "-------- epoch = 129/150 --------\n",
      "batch: 1; loss = 0.5385843515396118\n",
      "-------- loss = 0.5385843515396118 --------\n",
      "-------- epoch = 130/150 --------\n",
      "batch: 1; loss = 0.5386027693748474\n",
      "-------- loss = 0.5386027693748474 --------\n",
      "-------- epoch = 131/150 --------\n",
      "batch: 1; loss = 0.5386240482330322\n",
      "-------- loss = 0.5386240482330322 --------\n",
      "-------- epoch = 132/150 --------\n",
      "batch: 1; loss = 0.5386477708816528\n",
      "-------- loss = 0.5386477708816528 --------\n",
      "-------- epoch = 133/150 --------\n",
      "batch: 1; loss = 0.5386738777160645\n",
      "-------- loss = 0.5386738777160645 --------\n",
      "-------- epoch = 134/150 --------\n",
      "batch: 1; loss = 0.5387023091316223\n",
      "-------- loss = 0.5387023091316223 --------\n",
      "-------- epoch = 135/150 --------\n",
      "batch: 1; loss = 0.5387330055236816\n",
      "-------- loss = 0.5387330055236816 --------\n",
      "-------- epoch = 136/150 --------\n",
      "batch: 1; loss = 0.5387657284736633\n",
      "-------- loss = 0.5387657284736633 --------\n",
      "-------- epoch = 137/150 --------\n",
      "batch: 1; loss = 0.5388003587722778\n",
      "-------- loss = 0.5388003587722778 --------\n",
      "-------- epoch = 138/150 --------\n",
      "batch: 1; loss = 0.5388367772102356\n",
      "-------- loss = 0.5388367772102356 --------\n",
      "-------- epoch = 139/150 --------\n",
      "batch: 1; loss = 0.5388749837875366\n",
      "-------- loss = 0.5388749837875366 --------\n",
      "-------- epoch = 140/150 --------\n",
      "batch: 1; loss = 0.538914680480957\n",
      "-------- loss = 0.538914680480957 --------\n",
      "-------- epoch = 141/150 --------\n",
      "batch: 1; loss = 0.5389559268951416\n",
      "-------- loss = 0.5389559268951416 --------\n",
      "-------- epoch = 142/150 --------\n",
      "batch: 1; loss = 0.5389986038208008\n",
      "-------- loss = 0.5389986038208008 --------\n",
      "-------- epoch = 143/150 --------\n",
      "batch: 1; loss = 0.5390426516532898\n",
      "-------- loss = 0.5390426516532898 --------\n",
      "-------- epoch = 144/150 --------\n",
      "batch: 1; loss = 0.5390879511833191\n",
      "-------- loss = 0.5390879511833191 --------\n",
      "-------- epoch = 145/150 --------\n",
      "batch: 1; loss = 0.5391345620155334\n",
      "-------- loss = 0.5391345620155334 --------\n",
      "-------- epoch = 146/150 --------\n",
      "batch: 1; loss = 0.5391823649406433\n",
      "-------- loss = 0.5391823649406433 --------\n",
      "-------- epoch = 147/150 --------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch: 1; loss = 0.5392311215400696\n",
      "-------- loss = 0.5392311215400696 --------\n",
      "-------- epoch = 148/150 --------\n",
      "batch: 1; loss = 0.5392810106277466\n",
      "-------- loss = 0.5392810106277466 --------\n",
      "-------- epoch = 149/150 --------\n",
      "batch: 1; loss = 0.5393319129943848\n",
      "-------- loss = 0.5393319129943848 --------\n",
      "-------- epoch = 150/150 --------\n",
      "batch: 1; loss = 0.5393837690353394\n",
      "-------- loss = 0.5393837690353394 --------\n"
     ]
    }
   ],
   "source": [
    "model_tf_idf = Model_tf_idf()\n",
    "model_tf_idf.to(device)\n",
    "model_tf_idf.fit(data_tf_idf, batch_size, epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "75b3331b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "recall = 0.96875; precision = 0.33695652173913043\n"
     ]
    }
   ],
   "source": [
    "predict(model_tf_idf, data_tf_idf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "d9439dd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_tf_idf_test = DataPreprocessing_tf_idf(df_data, 'test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "ccd1408e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "recall = 0.8125; precision = 0.2549019607843137\n"
     ]
    }
   ],
   "source": [
    "predict(model_tf_idf, data_tf_idf_test)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
