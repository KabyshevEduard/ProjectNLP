{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2ebf1dcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "from sklearn import preprocessing\n",
    "from sklearn import metrics\n",
    "from sklearn import model_selection\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "#import natasha\n",
    "#from navec import Navec\n",
    "#from slovnet.model.emb import NavecEmbedding\n",
    "import numpy as np\n",
    "import nltk\n",
    "import string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "71197ae3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/eduard/nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
      "[nltk_data] Downloading package stopwords to /home/eduard/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#path = 'navec_hudlit_v1_12B_500K_300d_100q.tar'\n",
    "#navec = Navec.load(path)\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b7d93d9f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cpu')"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "01f8ce50",
   "metadata": {},
   "outputs": [],
   "source": [
    "#ДЛЯ NAVEC EMBEDDINGS\n",
    "class DataPreprocessing(torch.utils.data.Dataset):\n",
    "    def __init__(self, path, sep, encoding, subset='train'):\n",
    "        self.subset = subset\n",
    "        df = pd.read_csv(path, sep=sep, encoding=encoding)\n",
    "        self.length_of_vec = 0\n",
    "        for i in range(len(df)):\n",
    "            if len(df['Описание анкеты'].values[i].split(' ')) > self.length_of_vec:\n",
    "                self.length_of_vec = len(df['Описание анкеты'].values[i].split(' '))\n",
    "        #self.x_train, self.x_test, self.y_train, self.y_test = model_selection.train_test_split(self.df['Описание анкеты'], self.df[' Рекомендовать'], test_size=0.33, random_state=42)\n",
    "        df_t = df[df[' Рекомендовать'] == 1].iloc[0:16]\n",
    "        df_f = df[df[' Рекомендовать'] == 0].iloc[0:48]\n",
    "        df_test = pd.concat([df_t, df_f], ignore_index=True)\n",
    "        self.x_test = df_test['Описание анкеты']\n",
    "        self.y_test = df_test[' Рекомендовать']\n",
    "        train = df.drop(index=df_t.index.append(df_f.index))\n",
    "        self.x_train = train['Описание анкеты']\n",
    "        self.y_train = train[' Рекомендовать']\n",
    "        \n",
    "    def vectorize(self, disc):\n",
    "        tokens = nltk.word_tokenize(disc, language='russian')\n",
    "        tokens_without_punct = [str(token) for token in tokens if token not in string.punctuation]\n",
    "        without_stop_words = [str(token) for token in tokens_without_punct if token not in nltk.corpus.stopwords.words('russian')]\n",
    "        #snowball = nltk.stem.snowball.SnowballStemmer('russian')\n",
    "        #stemmed_tokens = [str(snowball.stem(i)) for i in without_stop_words] леммизация\n",
    "        ids = []\n",
    "        for j in without_stop_words:\n",
    "            if j.lower() in navec:\n",
    "                ids.append(navec.vocab[j.lower()])\n",
    "            else:\n",
    "                ids.append(navec.vocab['<unk>'])\n",
    "        if len(ids) < self.length_of_vec:\n",
    "            addition = [0 for i in range(self.length_of_vec - len(ids))]\n",
    "            ids += addition\n",
    "        return torch.tensor(ids) #device=device\n",
    "                \n",
    "    def __len__(self):\n",
    "        if self.subset == 'train':\n",
    "            return len(self.x_train)\n",
    "        else:\n",
    "            return len(self.x_test)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        if self.subset == 'train':\n",
    "            disc = self.x_train.values[index]\n",
    "            recomend = self.y_train.values[index].astype(float)\n",
    "        else:\n",
    "            disc = self.x_test.values[index]\n",
    "            recomend = self.y_test.values[index].astype(float)\n",
    "        if type(disc) is str:\n",
    "            return self.vectorize(disc), torch.tensor(recomend) #device=device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "71903f7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#path_df = 'C:/Users/Эдуард/Desktop/проект/data.txt' windows\n",
    "path_df = '/home/eduard/Рабочий стол/проект1/data.txt'\n",
    "sep = ';'\n",
    "encoding = 'cp1251'\n",
    "#data = DataPreprocessing(path_df, sep, encoding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f9ed5a89",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(path_df, sep=sep, encoding=encoding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "811b630c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 196 entries, 0 to 195\n",
      "Data columns (total 2 columns):\n",
      " #   Column           Non-Null Count  Dtype \n",
      "---  ------           --------------  ----- \n",
      " 0   Описание анкеты  196 non-null    object\n",
      " 1    Рекомендовать   196 non-null    int64 \n",
      "dtypes: int64(1), object(1)\n",
      "memory usage: 3.2+ KB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c1c2f470",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Описание анкеты</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Рекомендовать</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>148</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>48</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                Описание анкеты\n",
       " Рекомендовать                 \n",
       "0                           148\n",
       "1                            48"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.groupby(by=' Рекомендовать').count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a26bd8c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True: 16\n",
      "False: 48\n"
     ]
    }
   ],
   "source": [
    "print('True: 16')\n",
    "print('False: 48')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "fe393ef9",
   "metadata": {},
   "outputs": [],
   "source": [
    "dictionary = {}\n",
    "k = 0\n",
    "for i in range(len(df['Описание анкеты'].values)):\n",
    "    temp = df['Описание анкеты'].values[i].split(' ')\n",
    "    for j in temp:\n",
    "        if j.lower() in dictionary:\n",
    "            pass\n",
    "        else:\n",
    "            dictionary[j.lower()] = k\n",
    "            k += 1\n",
    "dictionary['<unk>'] = 955"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "98b751d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#ДЛЯ СВОИХ EMBEDDINGS\n",
    "class DataPreprocessing_self_emb(torch.utils.data.Dataset):\n",
    "    def __init__(self, path, sep, encoding, subset='train'):\n",
    "        self.subset = subset\n",
    "        df = pd.read_csv(path, sep=sep, encoding=encoding)\n",
    "        self.length_of_vec = 0\n",
    "        for i in range(len(df)):\n",
    "            if len(df['Описание анкеты'].values[i].split(' ')) > self.length_of_vec:\n",
    "                self.length_of_vec = len(df['Описание анкеты'].values[i].split(' '))\n",
    "        #self.x_train, self.x_test, self.y_train, self.y_test = model_selection.train_test_split(self.df['Описание анкеты'], self.df[' Рекомендовать'], test_size=0.33, random_state=42)\n",
    "        df_t = df[df[' Рекомендовать'] == 1].iloc[0:16]\n",
    "        df_f = df[df[' Рекомендовать'] == 0].iloc[0:48]\n",
    "        df_test = pd.concat([df_t, df_f], ignore_index=True)\n",
    "        self.x_test = df_test['Описание анкеты']\n",
    "        self.y_test = df_test[' Рекомендовать']\n",
    "        train = df.drop(index=df_t.index.append(df_f.index))\n",
    "        self.x_train = train['Описание анкеты']\n",
    "        self.y_train = train[' Рекомендовать']\n",
    "        \n",
    "    def vectorize(self, disc):\n",
    "        tokens = nltk.word_tokenize(disc, language='russian')\n",
    "        tokens_without_punct = [str(token) for token in tokens if token not in string.punctuation]\n",
    "        without_stop_words = [str(token) for token in tokens_without_punct if token not in nltk.corpus.stopwords.words('russian')]\n",
    "        #snowball = nltk.stem.snowball.SnowballStemmer('russian')\n",
    "        #stemmed_tokens = [str(snowball.stem(i)) for i in without_stop_words] леммизация\n",
    "        ids = []\n",
    "        for j in without_stop_words:\n",
    "            if j.lower() in dictionary:\n",
    "                ids.append(dictionary[j.lower()])\n",
    "            else:\n",
    "                ids.append(dictionary['<unk>'])\n",
    "        if len(ids) < self.length_of_vec:\n",
    "            addition = [0 for i in range(self.length_of_vec - len(ids))]\n",
    "            ids += addition\n",
    "        return torch.tensor(ids) #device=device\n",
    "                \n",
    "    def __len__(self):\n",
    "        if self.subset == 'train':\n",
    "            return len(self.x_train)\n",
    "        else:\n",
    "            return len(self.x_test)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        if self.subset == 'train':\n",
    "            disc = self.x_train.values[index]\n",
    "            recomend = self.y_train.values[index].astype(float)\n",
    "        else:\n",
    "            disc = self.x_test.values[index]\n",
    "            recomend = self.y_test.values[index].astype(float)\n",
    "        if type(disc) is str:\n",
    "            return self.vectorize(disc), torch.tensor(recomend) #device=device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "2b79610c",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = DataPreprocessing_self_emb(path_df, sep, encoding)\n",
    "batch_size = len(data)\n",
    "epochs = 100"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a98ef4b",
   "metadata": {},
   "source": [
    "Input matrix 44x300"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "00e689e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model(torch.nn.Module):\n",
    "    x, _ = data[0]\n",
    "    dim = len(x)\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.emb = torch.nn.Embedding(len(dictionary), 300)\n",
    "        self.rnn = torch.nn.LSTM(300, 100, 2, batch_first=True)\n",
    "        #self.sequential = torch.nn.Sequential(\n",
    "            #torch.nn.Conv1d(self.dim, self.dim, 3),\n",
    "            #torch.nn.Conv1d(self.dim, self.dim, 3),\n",
    "            #torch.nn.ELU(),\n",
    "            #torch.nn.MaxPool1d(2),\n",
    "            #\n",
    "            #torch.nn.Conv1d(self.dim, self.dim, 3),\n",
    "            #torch.nn.Conv1d(self.dim, self.dim, 3),\n",
    "            #torch.nn.ELU(),\n",
    "            #torch.nn.MaxPool1d(2),\n",
    "            #\n",
    "            #torch.nn.Conv1d(self.dim, self.dim, 3),\n",
    "            #torch.nn.Conv1d(self.dim, self.dim, 3),\n",
    "            #torch.nn.ELU(),\n",
    "            #torch.nn.MaxPool1d(2),\n",
    "            #\n",
    "            #torch.nn.Conv1d(self.dim, self.dim, 3),\n",
    "            #torch.nn.Conv1d(self.dim, self.dim, 3),\n",
    "            #torch.nn.ELU(),\n",
    "            #torch.nn.MaxPool1d(2),\n",
    "        #)\n",
    "        self.flat = torch.nn.Flatten()\n",
    "        self.lin1 = torch.nn.Linear(4400, 100)\n",
    "        self.lin2 = torch.nn.Linear(100, 1)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        embedded_x = self.emb(x)\n",
    "        y, (hidden, cell) = self.rnn(embedded_x)\n",
    "        #y = self.sequential(embedded_x)\n",
    "        y = self.flat(y)\n",
    "        y = torch.nn.functional.tanh(self.lin1(y))\n",
    "        y = torch.sigmoid(self.lin2(y))\n",
    "        return y\n",
    "    \n",
    "    def fit(self, data, batch_size, epochs):\n",
    "        dl = torch.utils.data.DataLoader(data, batch_size=batch_size)\n",
    "        loss_fn = torch.nn.BCELoss()\n",
    "        optimizer = torch.optim.Adam(self.parameters(), 0.01, weight_decay=0.05)\n",
    "        self.train()\n",
    "        for epoch in range(epochs):\n",
    "            Loss = 0\n",
    "            print(f'-------- epoch = {epoch+1}/{epochs} --------')\n",
    "            for batch, (X, Y) in enumerate(dl):\n",
    "                optimizer.zero_grad()\n",
    "                #pred = torch.tensor([model(X[i]) for i in range(len(X))], requires_grad=True)\n",
    "                #target = Y.to(torch.float32)\n",
    "                pred = self(X)\n",
    "                target = torch.reshape(Y, (-1, 1)).float()\n",
    "                loss = loss_fn(pred, target)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                #if batch == int(len(data)/batch_size):\n",
    "                    #loss = loss.item()\n",
    "                    #print(f'batch = {batch}; loss = {loss}')\n",
    "                if batch % 1 == 0:\n",
    "                    loss, current = loss.item(), (batch + 1)*len(X)\n",
    "                    Loss += loss\n",
    "                    print(f'batch: {batch+1}; loss = {loss}')\n",
    "            print(f'-------- loss = {Loss/(int(len(data)/batch_size))} --------')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "9cb2aefe",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Model()\n",
    "#model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "572edb84",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------- epoch = 1/100 --------\n",
      "batch: 1; loss = 0.7057065367698669\n",
      "-------- loss = 0.7057065367698669 --------\n",
      "-------- epoch = 2/100 --------\n",
      "batch: 1; loss = 1.0524306297302246\n",
      "-------- loss = 1.0524306297302246 --------\n",
      "-------- epoch = 3/100 --------\n",
      "batch: 1; loss = 2.861630439758301\n",
      "-------- loss = 2.861630439758301 --------\n",
      "-------- epoch = 4/100 --------\n",
      "batch: 1; loss = 0.6689994931221008\n",
      "-------- loss = 0.6689994931221008 --------\n",
      "-------- epoch = 5/100 --------\n",
      "batch: 1; loss = 0.6989892721176147\n",
      "-------- loss = 0.6989892721176147 --------\n",
      "-------- epoch = 6/100 --------\n",
      "batch: 1; loss = 0.5945637822151184\n",
      "-------- loss = 0.5945637822151184 --------\n",
      "-------- epoch = 7/100 --------\n",
      "batch: 1; loss = 0.5535013675689697\n",
      "-------- loss = 0.5535013675689697 --------\n",
      "-------- epoch = 8/100 --------\n",
      "batch: 1; loss = 0.5816926956176758\n",
      "-------- loss = 0.5816926956176758 --------\n",
      "-------- epoch = 9/100 --------\n",
      "batch: 1; loss = 0.5716347694396973\n",
      "-------- loss = 0.5716347694396973 --------\n",
      "-------- epoch = 10/100 --------\n",
      "batch: 1; loss = 0.5629681944847107\n",
      "-------- loss = 0.5629681944847107 --------\n",
      "-------- epoch = 11/100 --------\n",
      "batch: 1; loss = 0.5726979374885559\n",
      "-------- loss = 0.5726979374885559 --------\n",
      "-------- epoch = 12/100 --------\n",
      "batch: 1; loss = 0.5480766296386719\n",
      "-------- loss = 0.5480766296386719 --------\n",
      "-------- epoch = 13/100 --------\n",
      "batch: 1; loss = 0.5542973875999451\n",
      "-------- loss = 0.5542973875999451 --------\n",
      "-------- epoch = 14/100 --------\n",
      "batch: 1; loss = 0.5509338974952698\n",
      "-------- loss = 0.5509338974952698 --------\n",
      "-------- epoch = 15/100 --------\n",
      "batch: 1; loss = 0.5581783652305603\n",
      "-------- loss = 0.5581783652305603 --------\n",
      "-------- epoch = 16/100 --------\n",
      "batch: 1; loss = 0.5821139812469482\n",
      "-------- loss = 0.5821139812469482 --------\n",
      "-------- epoch = 17/100 --------\n",
      "batch: 1; loss = 0.5524323582649231\n",
      "-------- loss = 0.5524323582649231 --------\n",
      "-------- epoch = 18/100 --------\n",
      "batch: 1; loss = 0.5578426122665405\n",
      "-------- loss = 0.5578426122665405 --------\n",
      "-------- epoch = 19/100 --------\n",
      "batch: 1; loss = 0.5495828986167908\n",
      "-------- loss = 0.5495828986167908 --------\n",
      "-------- epoch = 20/100 --------\n",
      "batch: 1; loss = 0.5630776286125183\n",
      "-------- loss = 0.5630776286125183 --------\n",
      "-------- epoch = 21/100 --------\n",
      "batch: 1; loss = 0.549930214881897\n",
      "-------- loss = 0.549930214881897 --------\n",
      "-------- epoch = 22/100 --------\n",
      "batch: 1; loss = 0.5515326261520386\n",
      "-------- loss = 0.5515326261520386 --------\n",
      "-------- epoch = 23/100 --------\n",
      "batch: 1; loss = 0.567215085029602\n",
      "-------- loss = 0.567215085029602 --------\n",
      "-------- epoch = 24/100 --------\n",
      "batch: 1; loss = 0.5520777106285095\n",
      "-------- loss = 0.5520777106285095 --------\n",
      "-------- epoch = 25/100 --------\n",
      "batch: 1; loss = 0.5492396950721741\n",
      "-------- loss = 0.5492396950721741 --------\n",
      "-------- epoch = 26/100 --------\n",
      "batch: 1; loss = 0.5454993844032288\n",
      "-------- loss = 0.5454993844032288 --------\n",
      "-------- epoch = 27/100 --------\n",
      "batch: 1; loss = 0.5470986366271973\n",
      "-------- loss = 0.5470986366271973 --------\n",
      "-------- epoch = 28/100 --------\n",
      "batch: 1; loss = 0.5656828284263611\n",
      "-------- loss = 0.5656828284263611 --------\n",
      "-------- epoch = 29/100 --------\n",
      "batch: 1; loss = 0.6816635727882385\n",
      "-------- loss = 0.6816635727882385 --------\n",
      "-------- epoch = 30/100 --------\n",
      "batch: 1; loss = 0.6926321387290955\n",
      "-------- loss = 0.6926321387290955 --------\n",
      "-------- epoch = 31/100 --------\n",
      "batch: 1; loss = 0.5795674920082092\n",
      "-------- loss = 0.5795674920082092 --------\n",
      "-------- epoch = 32/100 --------\n",
      "batch: 1; loss = 0.8995009660720825\n",
      "-------- loss = 0.8995009660720825 --------\n",
      "-------- epoch = 33/100 --------\n",
      "batch: 1; loss = 0.5581424832344055\n",
      "-------- loss = 0.5581424832344055 --------\n",
      "-------- epoch = 34/100 --------\n",
      "batch: 1; loss = 0.553935170173645\n",
      "-------- loss = 0.553935170173645 --------\n",
      "-------- epoch = 35/100 --------\n",
      "batch: 1; loss = 0.5638216733932495\n",
      "-------- loss = 0.5638216733932495 --------\n",
      "-------- epoch = 36/100 --------\n",
      "batch: 1; loss = 0.5830914378166199\n",
      "-------- loss = 0.5830914378166199 --------\n",
      "-------- epoch = 37/100 --------\n",
      "batch: 1; loss = 0.5656251907348633\n",
      "-------- loss = 0.5656251907348633 --------\n",
      "-------- epoch = 38/100 --------\n",
      "batch: 1; loss = 0.5565992593765259\n",
      "-------- loss = 0.5565992593765259 --------\n",
      "-------- epoch = 39/100 --------\n",
      "batch: 1; loss = 0.5550165176391602\n",
      "-------- loss = 0.5550165176391602 --------\n",
      "-------- epoch = 40/100 --------\n",
      "batch: 1; loss = 0.5607829689979553\n",
      "-------- loss = 0.5607829689979553 --------\n",
      "-------- epoch = 41/100 --------\n",
      "batch: 1; loss = 0.5560871958732605\n",
      "-------- loss = 0.5560871958732605 --------\n",
      "-------- epoch = 42/100 --------\n",
      "batch: 1; loss = 0.5525935888290405\n",
      "-------- loss = 0.5525935888290405 --------\n",
      "-------- epoch = 43/100 --------\n",
      "batch: 1; loss = 0.5446687340736389\n",
      "-------- loss = 0.5446687340736389 --------\n",
      "-------- epoch = 44/100 --------\n",
      "batch: 1; loss = 0.5532172322273254\n",
      "-------- loss = 0.5532172322273254 --------\n",
      "-------- epoch = 45/100 --------\n",
      "batch: 1; loss = 0.5750618577003479\n",
      "-------- loss = 0.5750618577003479 --------\n",
      "-------- epoch = 46/100 --------\n",
      "batch: 1; loss = 0.5521807074546814\n",
      "-------- loss = 0.5521807074546814 --------\n",
      "-------- epoch = 47/100 --------\n",
      "batch: 1; loss = 0.5536994338035583\n",
      "-------- loss = 0.5536994338035583 --------\n",
      "-------- epoch = 48/100 --------\n",
      "batch: 1; loss = 0.5536462664604187\n",
      "-------- loss = 0.5536462664604187 --------\n",
      "-------- epoch = 49/100 --------\n",
      "batch: 1; loss = 0.5500262379646301\n",
      "-------- loss = 0.5500262379646301 --------\n",
      "-------- epoch = 50/100 --------\n",
      "batch: 1; loss = 0.5603084564208984\n",
      "-------- loss = 0.5603084564208984 --------\n",
      "-------- epoch = 51/100 --------\n",
      "batch: 1; loss = 0.5570021867752075\n",
      "-------- loss = 0.5570021867752075 --------\n",
      "-------- epoch = 52/100 --------\n",
      "batch: 1; loss = 0.5486035346984863\n",
      "-------- loss = 0.5486035346984863 --------\n",
      "-------- epoch = 53/100 --------\n",
      "batch: 1; loss = 0.5485227704048157\n",
      "-------- loss = 0.5485227704048157 --------\n",
      "-------- epoch = 54/100 --------\n",
      "batch: 1; loss = 0.5482637882232666\n",
      "-------- loss = 0.5482637882232666 --------\n",
      "-------- epoch = 55/100 --------\n",
      "batch: 1; loss = 0.5546349883079529\n",
      "-------- loss = 0.5546349883079529 --------\n",
      "-------- epoch = 56/100 --------\n",
      "batch: 1; loss = 0.550163984298706\n",
      "-------- loss = 0.550163984298706 --------\n",
      "-------- epoch = 57/100 --------\n",
      "batch: 1; loss = 0.5478267073631287\n",
      "-------- loss = 0.5478267073631287 --------\n",
      "-------- epoch = 58/100 --------\n",
      "batch: 1; loss = 0.5480804443359375\n",
      "-------- loss = 0.5480804443359375 --------\n",
      "-------- epoch = 59/100 --------\n",
      "batch: 1; loss = 0.552138090133667\n",
      "-------- loss = 0.552138090133667 --------\n",
      "-------- epoch = 60/100 --------\n",
      "batch: 1; loss = 0.5528182983398438\n",
      "-------- loss = 0.5528182983398438 --------\n",
      "-------- epoch = 61/100 --------\n",
      "batch: 1; loss = 0.5485714673995972\n",
      "-------- loss = 0.5485714673995972 --------\n",
      "-------- epoch = 62/100 --------\n",
      "batch: 1; loss = 0.5486931204795837\n",
      "-------- loss = 0.5486931204795837 --------\n",
      "-------- epoch = 63/100 --------\n",
      "batch: 1; loss = 0.5511903762817383\n",
      "-------- loss = 0.5511903762817383 --------\n",
      "-------- epoch = 64/100 --------\n",
      "batch: 1; loss = 0.5519810318946838\n",
      "-------- loss = 0.5519810318946838 --------\n",
      "-------- epoch = 65/100 --------\n",
      "batch: 1; loss = 0.5487874746322632\n",
      "-------- loss = 0.5487874746322632 --------\n",
      "-------- epoch = 66/100 --------\n",
      "batch: 1; loss = 0.5491217970848083\n",
      "-------- loss = 0.5491217970848083 --------\n",
      "-------- epoch = 67/100 --------\n",
      "batch: 1; loss = 0.5522476434707642\n",
      "-------- loss = 0.5522476434707642 --------\n",
      "-------- epoch = 68/100 --------\n",
      "batch: 1; loss = 0.5519209504127502\n",
      "-------- loss = 0.5519209504127502 --------\n",
      "-------- epoch = 69/100 --------\n",
      "batch: 1; loss = 0.550288736820221\n",
      "-------- loss = 0.550288736820221 --------\n",
      "-------- epoch = 70/100 --------\n",
      "batch: 1; loss = 0.5506600737571716\n",
      "-------- loss = 0.5506600737571716 --------\n",
      "-------- epoch = 71/100 --------\n",
      "batch: 1; loss = 0.5536815524101257\n",
      "-------- loss = 0.5536815524101257 --------\n",
      "-------- epoch = 72/100 --------\n",
      "batch: 1; loss = 0.5514715909957886\n",
      "-------- loss = 0.5514715909957886 --------\n",
      "-------- epoch = 73/100 --------\n",
      "batch: 1; loss = 0.5510597229003906\n",
      "-------- loss = 0.5510597229003906 --------\n",
      "-------- epoch = 74/100 --------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch: 1; loss = 0.5521799325942993\n",
      "-------- loss = 0.5521799325942993 --------\n",
      "-------- epoch = 75/100 --------\n",
      "batch: 1; loss = 0.5533760190010071\n",
      "-------- loss = 0.5533760190010071 --------\n",
      "-------- epoch = 76/100 --------\n",
      "batch: 1; loss = 0.5516107678413391\n",
      "-------- loss = 0.5516107678413391 --------\n",
      "-------- epoch = 77/100 --------\n",
      "batch: 1; loss = 0.5519434213638306\n",
      "-------- loss = 0.5519434213638306 --------\n",
      "-------- epoch = 78/100 --------\n",
      "batch: 1; loss = 0.554043173789978\n",
      "-------- loss = 0.554043173789978 --------\n",
      "-------- epoch = 79/100 --------\n",
      "batch: 1; loss = 0.5531467199325562\n",
      "-------- loss = 0.5531467199325562 --------\n",
      "-------- epoch = 80/100 --------\n",
      "batch: 1; loss = 0.5527438521385193\n",
      "-------- loss = 0.5527438521385193 --------\n",
      "-------- epoch = 81/100 --------\n",
      "batch: 1; loss = 0.5537112951278687\n",
      "-------- loss = 0.5537112951278687 --------\n",
      "-------- epoch = 82/100 --------\n",
      "batch: 1; loss = 0.5548765063285828\n",
      "-------- loss = 0.5548765063285828 --------\n",
      "-------- epoch = 83/100 --------\n",
      "batch: 1; loss = 0.5537558794021606\n",
      "-------- loss = 0.5537558794021606 --------\n",
      "-------- epoch = 84/100 --------\n",
      "batch: 1; loss = 0.5538346767425537\n",
      "-------- loss = 0.5538346767425537 --------\n",
      "-------- epoch = 85/100 --------\n",
      "batch: 1; loss = 0.5550487041473389\n",
      "-------- loss = 0.5550487041473389 --------\n",
      "-------- epoch = 86/100 --------\n",
      "batch: 1; loss = 0.555307149887085\n",
      "-------- loss = 0.555307149887085 --------\n",
      "-------- epoch = 87/100 --------\n",
      "batch: 1; loss = 0.5545327663421631\n",
      "-------- loss = 0.5545327663421631 --------\n",
      "-------- epoch = 88/100 --------\n",
      "batch: 1; loss = 0.5547664761543274\n",
      "-------- loss = 0.5547664761543274 --------\n",
      "-------- epoch = 89/100 --------\n",
      "batch: 1; loss = 0.5558023452758789\n",
      "-------- loss = 0.5558023452758789 --------\n",
      "-------- epoch = 90/100 --------\n",
      "batch: 1; loss = 0.55582594871521\n",
      "-------- loss = 0.55582594871521 --------\n",
      "-------- epoch = 91/100 --------\n",
      "batch: 1; loss = 0.5552273392677307\n",
      "-------- loss = 0.5552273392677307 --------\n",
      "-------- epoch = 92/100 --------\n",
      "batch: 1; loss = 0.5553989410400391\n",
      "-------- loss = 0.5553989410400391 --------\n",
      "-------- epoch = 93/100 --------\n",
      "batch: 1; loss = 0.5561798810958862\n",
      "-------- loss = 0.5561798810958862 --------\n",
      "-------- epoch = 94/100 --------\n",
      "batch: 1; loss = 0.5562890768051147\n",
      "-------- loss = 0.5562890768051147 --------\n",
      "-------- epoch = 95/100 --------\n",
      "batch: 1; loss = 0.5557512044906616\n",
      "-------- loss = 0.5557512044906616 --------\n",
      "-------- epoch = 96/100 --------\n",
      "batch: 1; loss = 0.5557252168655396\n",
      "-------- loss = 0.5557252168655396 --------\n",
      "-------- epoch = 97/100 --------\n",
      "batch: 1; loss = 0.5562556385993958\n",
      "-------- loss = 0.5562556385993958 --------\n",
      "-------- epoch = 98/100 --------\n",
      "batch: 1; loss = 0.5565448999404907\n",
      "-------- loss = 0.5565448999404907 --------\n",
      "-------- epoch = 99/100 --------\n",
      "batch: 1; loss = 0.5561258792877197\n",
      "-------- loss = 0.5561258792877197 --------\n",
      "-------- epoch = 100/100 --------\n",
      "batch: 1; loss = 0.5558643937110901\n",
      "-------- loss = 0.5558643937110901 --------\n"
     ]
    }
   ],
   "source": [
    "model.fit(data, batch_size, epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "9f481543",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(model, data, metric=True, i=0,):\n",
    "    if metric:\n",
    "        dl = torch.utils.data.DataLoader(data, batch_size=len(data))\n",
    "        for X, Y in dl:\n",
    "            Y_pred = []\n",
    "            arr = torch.reshape(model(X), (len(data), )).detach().numpy()\n",
    "            for i in arr:\n",
    "                if i >= 0.2:\n",
    "                    Y_pred.append(1)\n",
    "                elif i < 0.2:\n",
    "                    Y_pred.append(0) \n",
    "            rec = metrics.recall_score(Y.detach().numpy(), np.array(Y_pred))\n",
    "            prec = metrics.precision_score(Y.detach().numpy(), np.array(Y_pred))\n",
    "        print(f'recall = {rec}; precision = {prec}')\n",
    "    else:\n",
    "        x, y = data[i]\n",
    "        x = torch.reshape(x, (1, 44))\n",
    "        print(f'predicted = {model(x)}: true = {y}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "5d9edb8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model, 'model.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "4dcc4f60",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Model(\n",
       "  (emb): Embedding(956, 300)\n",
       "  (rnn): LSTM(300, 100, num_layers=2, batch_first=True)\n",
       "  (flat): Flatten(start_dim=1, end_dim=-1)\n",
       "  (lin1): Linear(in_features=4400, out_features=100, bias=True)\n",
       "  (lin2): Linear(in_features=100, out_features=1, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "200a2e58",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "recall = 1.0; precision = 0.24242424242424243\n"
     ]
    }
   ],
   "source": [
    "predict(model, data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "3d16757d",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_test = DataPreprocessing_self_emb(path_df, sep, encoding, subset='test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "f1f52ba8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "recall = 1.0; precision = 0.25\n"
     ]
    }
   ],
   "source": [
    "predict(model, data_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d749589",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
